# üìã Pipeline de Extra√ß√£o RAG para Leis - Documenta√ß√£o de Decis√µes

> **Projeto**: Sistema RAG para orgaos publicos
> **Data de Inicio**: 21/12/2024
> **Ultima Atualizacao**: 23/12/2024 21:30
> **Status**: Fase 5 - RAG Completo com Resposta LLM ‚úÖ (Answer Generator + Cita√ß√µes)

---

## üéØ Objetivo do Projeto

Desenvolver um sistema RAG (Retrieval-Augmented Generation) completo e comercializ√°vel para √≥rg√£os p√∫blicos brasileiros, come√ßando pela extra√ß√£o e indexa√ß√£o de documentos legais (leis, decretos, instru√ß√µes normativas).

---

## ‚öñÔ∏è Requisitos de Licenciamento

| Requisito           | Decis√£o                                        |
| ------------------- | ---------------------------------------------- |
| Licen√ßas permitidas | Apache 2.0, MIT, BSD, PostgreSQL License       |
| Licen√ßas proibidas  | GPL, AGPL, SSPL, propriet√°rias                 |
| Motivo              | Produto comercial para venda a √≥rg√£os p√∫blicos |

---

## üõ†Ô∏è Stack Tecnol√≥gica Definida

### Backend

| Componente    | Tecnologia | Vers√£o | Licen√ßa | Justificativa               |
| ------------- | ---------- | ------ | ------- | --------------------------- |
| Linguagem     | Python     | 3.11+  | PSF     | Expertise da equipe         |
| Framework API | FastAPI    | latest | MIT     | Performance, tipagem, async |
| Valida√ß√£o     | Pydantic   | v2     | MIT     | Integra√ß√£o nativa FastAPI   |

### Extra√ß√£o de Documentos

| Componente     | Tecnologia | Vers√£o | Licen√ßa    | Justificativa                             |
| -------------- | ---------- | ------ | ---------- | ----------------------------------------- |
| Parser PDF     | Docling    | 2.15+  | MIT (IBM)  | Preserva hierarquia, markdown estruturado |
| OCR (fallback) | PaddleOCR  | latest | Apache 2.0 | Multil√≠ngue, portugu√™s                    |

### Framework Ag√™ntico

| Componente   | Tecnologia | Vers√£o | Licen√ßa | Justificativa                             |
| ------------ | ---------- | ------ | ------- | ----------------------------------------- |
| Orquestra√ß√£o | LangGraph  | 0.2+   | MIT     | Grafos de estado, checkpointing, flex√≠vel |

### LLM

| Componente      | Tecnologia        | Vers√£o  | Licen√ßa    | Justificativa                                   |
| --------------- | ----------------- | ------- | ---------- | ----------------------------------------------- |
| LLM (unico)     | **Qwen 3 8B-AWQ** | latest  | Apache 2.0 | Extracao + Enriquecimento (modelo unico)        |
| Runtime Prod    | **vLLM**          | 0.13+   | Apache 2.0 | Docker, API OpenAI-compatible, quantizacao AWQ  |
| Hardware        | GPU 12GB          | -       | -          | 8B-AWQ: 5.7GB + BGE-M3: ~2GB = ~8GB total       |

**Decisao**: Usar **modelo unico Qwen 3 8B-AWQ** para todas as tarefas.

> **Atualizado 22/12/2024**: Abandonamos a estrategia de model swapping (trocar 4B/8B entre fases).
> O ganho de velocidade do 4B nao justifica a complexidade operacional em producao.

**Por que modelo unico?**

| Criterio | Model Swapping (4B+8B) | Modelo Unico (8B) |
|----------|------------------------|-------------------|
| Complexidade | Alta (scripts de troca) | Baixa |
| Downtime | Sim (durante troca) | Nao |
| Race conditions | Possiveis | Nenhuma |
| Filas/workers | Complexo | Simples |
| Velocidade enriquecimento | 7.4s/chunk | 14.5s/chunk |
| Qualidade | Igual | Igual |

**Conclusao**: O 8B e 2x mais lento no enriquecimento, mas a simplicidade operacional
compensa. Em producao com filas (Redis/Celery), a latencia extra e absorvida pelo paralelismo.

**Configuracao vLLM Producao**:
```bash
docker run -d --name vllm --gpus all \
  -v huggingface-cache:/root/.cache/huggingface \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen3-8B-AWQ \
  --max-model-len 16000 \
  --gpu-memory-utilization 0.9
```

**Vantagens vLLM em Producao**:

- Continuous batching (maior throughput)
- PagedAttention (uso eficiente de VRAM)
- API compativel com OpenAI (facil migracao)
- Tensor parallelism para multiplas GPUs
- Quantizacao nativa (AWQ, GPTQ)

**Justificativa do modelo** (21/12/2024 - apos benchmarks extensivos):

- **8B-AWQ**: Unico modelo local que extraiu corretamente alineas (sub_items)
- 256K de contexto (8x mais que Qwen 2.5)
- Licenca Apache 2.0 (100% comercial)
- Forte em portugues juridico
- VRAM: 5.7GB (cabe em GPU 12GB com folga para BGE-M3)

**JSON Schema (Structured Output)** - Implementado 22/12/2024:

O vLLM suporta `response_format` com `json_schema` para forcar o modelo a gerar
apenas JSON valido seguindo um schema. Isso e usado na extracao (MD‚ÜíJSON) para
prevenir alucinacoes e garantir output estruturado.

```python
# Exemplo de uso no VLLMClient
result = client.chat_with_schema(
    messages=[{"role": "user", "content": "Extraia..."}],
    schema=LegalDocument,  # Pydantic model ou dict
    temperature=0.0,
)
# result ja e dict validado, nao string
```

| Fase | Usa json_schema? | Motivo |
|------|------------------|--------|
| Extracao (MD‚ÜíJSON) | **Sim** | Precisa de JSON estruturado exato |
| Enriquecimento | Nao | Retorna texto livre (context, thesis) |
| Resposta usuario | Nao | Retorna texto natural |

**Configuracao**:
```python
config = ExtractConfig.for_legal_documents()
config.llm.use_guided_json = True  # Habilita json_schema na extracao
```

**Beneficios**:
- Elimina parsing manual de JSON
- Previne JSON malformado
- Reduz alucinacoes de estrutura
- 100% de sucesso em testes

### Embeddings & Reranking

| Componente | Tecnologia             | Vers√£o | Licen√ßa    | Justificativa                               |
| ---------- | ---------------------- | ------ | ---------- | ------------------------------------------- |
| Embedding  | **BGE-M3**             | latest | Apache 2.0 | Multil√≠ngue, 8k contexto, h√≠brido           |
| Reranker   | **bge-reranker-v2-m3** | latest | Apache 2.0 | Cross-encoder multil√≠ngue, melhora precis√£o |
| Runtime    | FlagEmbedding          | latest | Apache 2.0 | Biblioteca oficial BAAI                     |

**IMPORTANTE: Onde cada componente roda**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        COMPUTADOR                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Docker Container  ‚îÇ     ‚îÇ      Python Local           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ       (vLLM)        ‚îÇ     ‚îÇ   (FlagEmbedding)           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                     ‚îÇ     ‚îÇ                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  Qwen 4B/8B (LLM)   ‚îÇ     ‚îÇ  BGE-M3 (embeddings)        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  API: localhost:8000‚îÇ     ‚îÇ  BGE-Reranker (rerank)      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ           ‚îÇ                              ‚îÇ                      ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                      ‚ñº                                          ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                  ‚îÇ
‚îÇ              ‚îÇ  GPU (VRAM)   ‚îÇ  ‚Üê Compartilham GPU              ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

- **vLLM (Docker)**: Serve LLMs via API HTTP, roda no container
- **FlagEmbedding (Local)**: Carrega BGE-M3 e Reranker diretamente no Python

**Estrategia de Retrieval 2-Stage** (Testado 22/12/2024):

```
Query ‚Üí BGE-M3 (Stage 1) ‚Üí Top 10 ‚Üí BGE-Reranker (Stage 2) ‚Üí Top 3
           ‚Üì                              ‚Üì
     Busca rapida              Reordenacao precisa
     (bi-encoder)              (cross-encoder)
```

| Stage | Componente | Funcao | Velocidade |
|-------|------------|--------|------------|
| 1 | BGE-M3 (dense + sparse) | Busca inicial no Milvus | Rapido |
| 2 | BGE-Reranker-v2-m3 | Reordena resultados | Lento mas preciso |

**Benchmark 2-Stage Retrieval** (22/12/2024):

| Query | Stage 1 Top 1 | Stage 2 Top 1 | Melhoria |
|-------|---------------|---------------|----------|
| "O que e ETP?" | Art-6 (0.032) | Art-3 (0.80) | Promoveu 5‚Üí1 |
| "ETP dispensado?" | Art-14 (0.033) | Art-14 (0.98) | Confirmou 98% |
| "contratacoes correlatas" | Art-9 (0.032) | Art-3 (0.87) | Promoveu 3‚Üí1 |

O reranker **corrige** o ranking inicial, promovendo documentos relevantes.

**Benchmark ColBERT vs Cross-Encoder** (22/12/2024):

Testamos dois metodos de reranking para documentos juridicos:

| Metodo | Tecnica | Score Medio | Concordancia |
|--------|---------|-------------|--------------|
| **Cross-Encoder** (BGE-Reranker) | Query+Doc juntos | **0.91** | 80% |
| ColBERT (MaxSim) | Late interaction | 0.62 | 80% |

Resultados por query:

| Query | Cross-Encoder | ColBERT | Acordo |
|-------|---------------|---------|--------|
| "O que e ETP?" | Art-3 (0.80) | Art-3 (0.62) | ‚úì |
| "Quando ETP dispensado?" | Art-14 (0.98) | Art-14 (0.63) | ‚úì |
| "contratacoes interdependentes" | Art-3 (0.87) | Art-3 (0.47) | ‚úì |
| "responsaveis elaboracao ETP" | Art-8 (0.97) | Art-8 (0.67) | ‚úì |
| "sistema ETP digital funciona?" | Art-17 (0.96) | Art-4 (0.72) | ‚úó |

**Decisao**: Manter **Cross-Encoder (BGE-Reranker)** como reranker principal:
- Scores mais altos e discriminativos (0.91 vs 0.62)
- 80% de concordancia com ColBERT
- Velocidade similar em producao

**ColBERT** disponivel como alternativa para queries com termos tecnicos exatos.

**Campos de Enriquecimento** (Contextual Retrieval):

| Campo | Descricao | Usado em |
|-------|-----------|----------|
| `text` | Texto original do artigo | **Reranking** (Stage 2) |
| `enriched_text` | Contexto + texto + perguntas | **Embedding** (dense_vector) |
| `context_header` | Frase contextualizando o artigo | enriched_text |
| `thesis_text` | Resumo do que o artigo determina | **Embedding** (thesis_vector) |
| `thesis_type` | Tipo: definicao, procedimento, etc | Filtro |
| `synthetic_questions` | Perguntas que o artigo responde | enriched_text |

> **IMPORTANTE (Corrigido 22/12/2024)**: O reranker usa `text` (original), NAO `enriched_text`.
> O prefixo `[CONTEXTO: ...]` do enriched_text dilui a relevancia para o cross-encoder.
> Testes mostraram: texto original = score 0.55, enriched_text = score 0.27.

**Estrategia de uso dos campos**:

| Stage | Campo Usado | Motivo |
|-------|-------------|--------|
| Stage 1 (Embedding) | `enriched_text` | Contexto extra melhora busca semantica |
| Stage 2 (Reranking) | `text` | Cross-encoder precisa de texto limpo |

O `enriched_text` combina todos os campos para melhor recuperacao semantica:
```
[CONTEXTO: Este artigo da IN 58/2022 define os conceitos basicos...]

Art. 3 Para fins do disposto nesta Instrucao Normativa, considera-se:
I - Estudo Tecnico Preliminar - ETP: documento constitutivo...

[PERGUNTAS RELACIONADAS:
- Qual e a funcao do Sistema ETP Digital?
- Quem assume a funcao de requisitante?]
```

### Armazenamento

| Componente   | Tecnologia            | Vers√£o | Licen√ßa    | Justificativa             |
| ------------ | --------------------- | ------ | ---------- | ------------------------- |
| Vector Store | Milvus Standalone     | 2.6    | Apache 2.0 | J√° em produ√ß√£o, escal√°vel |
| Collection   | Especializada em leis | -      | -          | Otimiza√ß√£o de schema      |

### Infraestrutura Docker (Produ√ß√£o)

```yaml
# docker-compose.prod.yml
services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --max-model-len 32768
      --gpu-memory-utilization 0.9
      --dtype auto
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  milvus:
    image: milvusdb/milvus:v2.6-latest
    # ... configura√ß√£o existente
```

### Frontend (Futuro)

| Componente | Tecnologia   | Vers√£o | Licen√ßa | Justificativa    |
| ---------- | ------------ | ------ | ------- | ---------------- |
| Framework  | Next.js      | 14+    | MIT     | SSR, performance |
| UI Library | React        | 18+    | MIT     | Padr√£o mercado   |
| Styling    | Tailwind CSS | 3+     | MIT     | Utility-first    |
| Components | shadcn/ui    | latest | MIT     | Customiz√°vel     |
| Icons      | Lucide React | latest | ISC     | Leve, moderno    |

---

## üìä Decis√µes de Arquitetura

### 1. Arquitetura de Extra√ß√£o (3 Abordagens Testadas)

#### üèÜ Resultado do Benchmark (21/12/2024)

| M√©trica                 | Extractor Simples | LangGraph Pipeline |   H√≠brido   |
| ----------------------- | :---------------: | :----------------: | :---------: |
| **Cap√≠tulos Corretos**  |       4 ‚úÖ        |        2 ‚ùå        |    4 ‚úÖ     |
| **Total Artigos**       |       19 ‚úÖ       |       19 ‚úÖ        |    19 ‚úÖ    |
| **Artigos Soltos**      |       0 ‚úÖ        |        3 ‚ùå        |    0 ‚úÖ     |
| **Schema Correto**      |       OK ‚úÖ       |      ERRO ‚ùå       |    OK ‚úÖ    |
| **Sub-items (al√≠neas)** |      SIM ‚úÖ       |       N√ÉO ‚ùå       |   SIM ‚úÖ    |
| **Metadados Completos** |      SIM ‚úÖ       |       N√ÉO ‚ùå       |   SIM ‚úÖ    |
| **SCORE TOTAL**         |    **100%** üèÜ    |      **30%**       | **100%** üèÜ |

#### Insight Importante

O **Extractor Simples** e o **H√≠brido** tiveram resultados **ID√äNTICOS** (100%). Isso prova que:

1. **O Extractor Simples √© o motor principal** - ele faz o trabalho pesado
2. **O Pydantic Schema √© a chave** - guia o LLM perfeitamente (similar ao LlamaExtract)
3. **LangGraph √© orquestrador, n√£o extrator** - n√£o melhora qualidade, apenas gerencia fluxo

#### Quando Usar Cada Abordagem

| Cen√°rio                     | Recomenda√ß√£o      |
| --------------------------- | ----------------- |
| **Scripts r√°pidos**         | Extractor Simples |
| **APIs/Microservices**      | Extractor Simples |
| **Prototipagem**            | Extractor Simples |
| **Produ√ß√£o robusta**        | Pipeline H√≠brido  |
| **Multi-documento**         | Pipeline H√≠brido  |
| **Com retry/checkpointing** | Pipeline H√≠brido  |

### 2. API de Extra√ß√£o (Estilo LlamaExtract)

Criamos uma API elegante inspirada no [LlamaExtract](https://developers.llamaindex.ai/python/cloud/llamaextract/), mas 100% open-source.

```python
from extract import Extractor, ExtractConfig
from models.legal_document import LegalDocument

# Extra√ß√£o simples
extractor = Extractor()
result = extractor.extract("documento.pdf", schema=LegalDocument)
print(result.data)

# Com configura√ß√£o customizada
config = ExtractConfig.for_legal_documents()
result = extractor.extract("lei.pdf", schema=LegalDocument, config=config)
```

#### M√≥dulos Criados

```
extracao/src/
‚îú‚îÄ‚îÄ extract/                    # API de extra√ß√£o (estilo LlamaExtract)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # ExtractConfig, ExtractMode, ChunkMode
‚îÇ   ‚îî‚îÄ‚îÄ extractor.py            # Extractor, ExtractionAgent, ExtractionResult
‚îú‚îÄ‚îÄ models/                     # Schemas Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ legal_document.py       # LegalDocument, Chapter, Article, etc.
‚îú‚îÄ‚îÄ pipeline/                   # Pipeline LangGraph
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ hybrid_pipeline.py      # Pipeline h√≠brido (LangGraph + Extractor)
‚îî‚îÄ‚îÄ agents/                     # Agentes LangGraph (legado)
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ pipeline_agent.py       # Pipeline LangGraph original
```

### 3. Estrat√©gia de Extra√ß√£o Final

```
PDF ‚Üí Docling (Markdown) ‚Üí Extractor (Pydantic + Qwen 3 8B) ‚Üí BGE-M3 (Embedding) ‚Üí Milvus
```

**Decis√£o**: Pipeline com **Extractor Simples** como motor principal.

**Fluxo detalhado**:

1. **Docling** extrai PDF ‚Üí Markdown estruturado
2. **Extractor** (Pydantic + Qwen 3 8B) ‚Üí JSON estruturado validado
3. **BGE-M3** ‚Üí Gera embeddings dos chunks
4. **Milvus** ‚Üí Armazena vetores e metadados

**Por que o Extractor Simples venceu**:

- Schema Pydantic no prompt = LLM sabe exatamente o que gerar
- Uma chamada focada > M√∫ltiplas chamadas gen√©ricas
- Menos complexidade = Menos erros
- Valida√ß√£o Pydantic integrada

### 4. Schema Pydantic para Documentos Legais

```python
class LegalDocument(BaseModel):
    """Modelo principal para documentos legais brasileiros."""

    document_type: str = Field(..., description="LEI, DECRETO, etc")
    issuing_body: str = Field(..., description="Nome do √≥rg√£o emissor")
    issuing_body_acronym: Optional[str] = Field(None, description="Sigla")
    number: str = Field(..., description="N√∫mero do documento")
    date: str = Field(..., description="Data YYYY-MM-DD")
    ementa: str = Field(..., description="Resumo oficial")
    publication_details: Optional[PublicationDetails] = None
    chapters: list[Chapter] = Field(..., min_length=1)
    signatory: Optional[str] = None

class Chapter(BaseModel):
    chapter_number: Optional[str] = Field(None, examples=["I", "II"])
    title: str
    articles: list[Article] = Field(..., min_length=1)

class Article(BaseModel):
    article_number: str = Field(..., examples=["1", "2", "10"])
    title: Optional[str] = None
    content: str
    items: list[Item] = Field(default_factory=list)
    paragraphs: list[Paragraph] = Field(default_factory=list)

class Item(BaseModel):
    item_identifier: str = Field(..., examples=["I", "II", "III"])
    description: str
    sub_items: list[SubItem] = Field(default_factory=list)

class SubItem(BaseModel):
    item_identifier: str = Field(..., examples=["a", "b", "c"])
    description: str

class Paragraph(BaseModel):
    paragraph_identifier: str = Field(..., examples=["1", "2", "unico"])
    content: str
```

### 5. Configura√ß√£o de Extra√ß√£o

```python
class ExtractConfig(BaseModel):
    """Configura√ß√£o de extra√ß√£o (similar ao LlamaExtract)."""

    extraction_mode: ExtractMode = ExtractMode.BALANCED
    extraction_target: ExtractTarget = ExtractTarget.PER_DOC
    chunk_mode: ChunkMode = ChunkMode.SECTION
    system_prompt: Optional[str] = None
    llm: LLMConfig = Field(default_factory=LLMConfig)
    validation: ValidationConfig = Field(default_factory=ValidationConfig)

    @classmethod
    def for_legal_documents(cls) -> "ExtractConfig":
        """Preset otimizado para documentos legais brasileiros."""
        return cls(
            extraction_mode=ExtractMode.BALANCED,
            chunk_mode=ChunkMode.ARTICLE,
            system_prompt="Especialista em documentos legais brasileiros...",
            llm=LLMConfig(model="qwen3:8b", temperature=0.0),
            validation=ValidationConfig(min_quality_score=0.98),
        )
```

### 6. Estrat√©gia de Chunking

**Abordagem**: Chunking Ag√™ntico Hier√°rquico

**Regras**:

1. Nunca separar inciso do seu artigo pai
2. Manter contexto hier√°rquico em cada chunk
3. Tamanho alvo: 500-1000 tokens
4. Parent-child linking para expans√£o de contexto

**Estrutura de Chunk**:

```python
{
    "id": "uuid",
    "law_id": "lei-12345-2024",
    "content": "Texto do chunk",
    "content_type": "artigo | paragrafo | inciso",
    "hierarchy": {
        "lei": "Lei n¬∫ 12.345/2024",
        "capitulo": "Cap√≠tulo I",
        "secao": "Se√ß√£o II",
        "artigo": "Art. 5¬∫"
    },
    "parent_context": "Contexto do pai",
    "metadata": {
        "position": 45,
        "tokens": 234,
        "keywords": ["direito", "cidad√£o"]
    }
}
```

### 7. Collection Milvus para Leis

**Nome**: `leis_v3` (atual) | `leis_v2` (legado, dropada)

**Schema v3** (30 campos com parent-child):

| Campo | Tipo | Indice | Descricao |
|-------|------|--------|-----------|
| id | INT64 | Primary | Auto-gerado |
| chunk_id | VARCHAR(200) | - | ID completo: IN-65-2021#ART-005 |
| **parent_chunk_id** | VARCHAR(200) | INVERTED | ID do chunk pai (vazio para artigos) |
| **span_id** | VARCHAR(100) | - | ART-005, PAR-005-1, INC-005-I |
| **device_type** | VARCHAR(32) | INVERTED | article, paragraph, inciso, alinea |
| **chunk_level** | VARCHAR(32) | - | article, device |
| text | VARCHAR(65535) | - | Texto original |
| enriched_text | VARCHAR(65535) | - | Contexto + texto + perguntas |
| dense_vector | FLOAT_VECTOR(1024) | HNSW | Embedding do enriched_text |
| thesis_vector | FLOAT_VECTOR(1024) | HNSW | Embedding do thesis_text |
| sparse_vector | SPARSE_FLOAT_VECTOR | SPARSE_INVERTED | Learned sparse BGE-M3 |
| context_header | VARCHAR(2000) | - | Frase de contexto |
| thesis_text | VARCHAR(5000) | - | Resumo do artigo |
| thesis_type | VARCHAR(100) | - | definicao, procedimento, etc |
| synthetic_questions | VARCHAR(10000) | - | Perguntas relacionadas |
| **citations** | VARCHAR(5000) | - | JSON: [span_id, ...] |
| document_id | VARCHAR(200) | - | ID √∫nico do documento |
| tipo_documento | VARCHAR(64) | INVERTED | LEI, DECRETO, IN |
| numero | VARCHAR(32) | - | N√∫mero do documento |
| ano | INT64 | INVERTED | Ano do documento |
| article_number | VARCHAR(32) | INVERTED | Numero do artigo |
| **schema_version** | VARCHAR(32) | - | Vers√£o do schema (1.0.0) |
| **extractor_version** | VARCHAR(32) | - | Vers√£o do extrator |
| **ingestion_timestamp** | VARCHAR(64) | - | Timestamp ISO |
| **document_hash** | VARCHAR(128) | - | SHA-256 do PDF |
| **page** | INT64 | - | P√°gina no PDF |
| **bbox_left/top/right/bottom** | FLOAT | - | Bounding box |

**Campos novos em negrito** (v3):
- Parent-child: `parent_chunk_id`, `span_id`, `device_type`, `chunk_level`, `citations`
- Proveni√™ncia: `schema_version`, `extractor_version`, `ingestion_timestamp`, `document_hash`
- Page spans: `page`, `bbox_left`, `bbox_top`, `bbox_right`, `bbox_bottom`

**Indices para Busca Hibrida**:
- `dense_vector`: HNSW (COSINE, M=16, efConstruction=256)
- `thesis_vector`: HNSW (COSINE, M=16, efConstruction=256)
- `sparse_vector`: SPARSE_INVERTED_INDEX (IP, drop_ratio=0.2)
- `parent_chunk_id`: INVERTED (para buscar filhos)
- `device_type`: INVERTED (filtrar por tipo)

### 8. Arquitetura Span-Based (23/12/2024)

**Abordagem**: Extra√ß√£o baseada em spans com hierarquia preservada.

A arquitetura span-based divide o documento em spans identificados por IDs √∫nicos
que preservam a estrutura hier√°rquica do documento legal.

**Componentes Principais**:

```
src/parsing/
‚îú‚îÄ‚îÄ span_parser.py              # SpanParser - parseia Markdown para spans
‚îú‚îÄ‚îÄ span_models.py              # Span, SpanType, ParsedDocument
‚îú‚îÄ‚îÄ span_extraction_models.py   # ArticleSpans (schema para LLM)
‚îî‚îÄ‚îÄ article_orchestrator.py     # ArticleOrchestrator (extra√ß√£o por artigo)
```

**Fluxo de Extra√ß√£o**:

```
PDF ‚Üí Docling ‚Üí Markdown ‚Üí SpanParser ‚Üí ParsedDocument
                                              ‚îÇ
                                              ‚ñº
                           ArticleOrchestrator (por artigo)
                                              ‚îÇ
                                              ‚ñº
                             ChunkMaterializer ‚Üí MaterializedChunk
                                              ‚îÇ
                                              ‚ñº
                                           Milvus
```

**Formato de Span IDs**:

| Tipo | Formato | Exemplo |
|------|---------|---------|
| Artigo | `ART-{nnn}` | `ART-005` |
| Par√°grafo | `PAR-{art}-{n}` | `PAR-005-1`, `PAR-005-UNICO` |
| Inciso | `INC-{art}-{romano}` | `INC-005-I`, `INC-005-II` |
| Al√≠nea | `ALI-{art}-{romano}-{letra}` | `ALI-005-I-a` |
| Inciso de ¬ß | `INC-{art}-{romano}_{par}` | `INC-005-I_2` (inciso I do ¬ß2) |

**Caracter√≠sticas**:

- **Curto-circuito**: Artigos sem filhos n√£o chamam LLM (economia de tokens)
- **Schema enum din√¢mico**: IDs permitidos s√£o passados como enum por artigo
- **Retry focado por janela**: Retry espec√≠fico para PAR ou INC, n√£o ambos
- **Valida√ß√£o de parent consistency**: `INC-005-I_2` deve ter parent `PAR-005-2`

**Resultados do Teste** (IN 65/2021):

| M√©trica | Valor |
|---------|-------|
| Artigos processados | 11/11 (100%) |
| Artigos v√°lidos | 11/11 |
| Total de chunks | 47 |
| ARTICLE chunks | 11 |
| PARAGRAPH chunks | 19 |
| INCISO chunks | 17 |

### 9. Parent-Child Retrieval com ChunkMaterializer (23/12/2024)

O `ChunkMaterializer` transforma ArticleChunks em chunks index√°veis com suporte
a parent-child retrieval.

**Estrutura de Chunks**:

```
Chunk Pai (ARTICLE)           Chunks Filhos
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ IN-65-2021#ART-005  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ IN-65-2021#PAR-005-1‚îÇ
‚îÇ parent_chunk_id: "" ‚îÇ       ‚îÇ parent: ART-005     ‚îÇ
‚îÇ type: ARTICLE       ‚îÇ       ‚îÇ type: PARAGRAPH     ‚îÇ
‚îÇ text: "Art. 5..."   ‚îÇ       ‚îÇ text: "¬ß1 ..."      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                              ‚îÇ IN-65-2021#INC-005-I‚îÇ
                              ‚îÇ parent: ART-005     ‚îÇ
                              ‚îÇ type: INCISO        ‚îÇ
                              ‚îÇ text: "I - ..."     ‚îÇ
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Estrat√©gia de Busca Parent-Child**:

```
Query ‚Üí Busca chunks filhos (INC/PAR) ‚Üí Agrega chunks pai ‚Üí Contexto expandido ‚Üí LLM
```

1. Busca sem√¢ntica retorna chunk filho (ex: `INC-005-II`)
2. Sistema recupera chunk pai via `parent_chunk_id` (ex: `ART-005`)
3. Contexto expandido passa para LLM (pai + filho + irm√£os relevantes)
4. LLM responde com contexto completo do artigo

**Classes do ChunkMaterializer**:

```python
@dataclass
class ChunkMetadata:
    """Metadados de proveni√™ncia e vers√£o."""
    schema_version: str = "1.0.0"
    extractor_version: str = "1.0.0"
    ingestion_timestamp: str
    document_hash: str  # SHA-256 do PDF
    valid_from: Optional[str]  # Vig√™ncia
    valid_to: Optional[str]
    page_spans: dict  # Coordenadas PDF (futuro)

@dataclass
class MaterializedChunk:
    """Chunk pronto para indexa√ß√£o."""
    chunk_id: str           # Ex: "IN-65-2021#ART-005"
    parent_chunk_id: str    # Ex: "" (pai) ou "IN-65-2021#ART-005" (filho)
    span_id: str            # Ex: "ART-005"
    device_type: DeviceType # ARTICLE, PARAGRAPH, INCISO, ALINEA
    chunk_level: ChunkLevel
    text: str
    citations: list[str]    # Spans que comp√µem este chunk
    metadata: ChunkMetadata
```

**Campos Din√¢micos para Milvus**:

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `parent_chunk_id` | VARCHAR | ID do chunk pai ("" se for artigo) |
| `span_id` | VARCHAR | ID do span (ex: "ART-005") |
| `device_type` | VARCHAR | article, paragraph, inciso, alinea |
| `citations` | JSON | Lista de span_ids que comp√µem o chunk |

### 10. Answer-JSON para Frontend (23/12/2024)

Formato estruturado de resposta para o frontend consumir.

**M√≥dulo**: `src/rag/answer_models.py`

**Estrutura da Resposta**:

```json
{
    "success": true,
    "data": {
        "answer": "Texto da resposta gerada pelo LLM...",
        "confidence": 0.92,
        "citations": [
            {
                "span_id": "ART-005",
                "chunk_id": "IN-65-2021#ART-005",
                "text": "Art. 5¬∫ O estudo...",
                "relevance": 0.95,
                "location": {"page": 2, "x": 50, "y": 100}
            }
        ],
        "sources": [
            {
                "document_id": "IN-65-2021",
                "title": "IN SEGES N¬∫ 65/2021",
                "tipo_documento": "INSTRUCAO NORMATIVA"
            }
        ]
    },
    "metadata": {
        "model": "Qwen/Qwen3-8B-AWQ",
        "latency_ms": 1234,
        "tokens_used": 456,
        "chunks_retrieved": 5,
        "chunks_used": 3,
        "timestamp": "2024-12-23T14:30:00Z"
    }
}
```

**C√°lculo de Confian√ßa**:

```python
def calculate_confidence(citations: list[Citation]) -> float:
    """
    F√≥rmula:
    - Base: m√©dia ponderada das relev√¢ncias (peso = relev√¢ncia¬≤)
    - Penalidade: se menos de 2 cita√ß√µes, reduz 20%
    - Bonus: se top cita√ß√£o > 0.9, adiciona 5%
    """
```

**Classes Principais**:

| Classe | Descri√ß√£o |
|--------|-----------|
| `Citation` | Uma cita√ß√£o espec√≠fica (span_id, texto, relev√¢ncia, localiza√ß√£o) |
| `Source` | Documento fonte (document_id, t√≠tulo, tipo) |
| `AnswerMetadata` | M√©tricas de debugging (modelo, lat√™ncia, tokens) |
| `AnswerResponse` | Resposta completa para frontend |
| `QueryRequest` | Request do frontend (query, filtros, top_k) |

### 11. Page Spans - Cita√ß√µes Visuais (23/12/2024)

M√≥dulo para extrair coordenadas PDF do Docling e mapear para spans do SpanParser.

**M√≥dulo**: `src/parsing/page_spans.py`

**Estrutura de Coordenadas**:

```python
@dataclass
class BoundingBox:
    left: float      # Coordenada X esquerda
    top: float       # Coordenada Y topo
    right: float     # Coordenada X direita
    bottom: float    # Coordenada Y base
    page: int        # N√∫mero da p√°gina
    coord_origin: str = "TOPLEFT"

@dataclass
class SpanLocation:
    span_id: str     # Ex: "ART-005"
    page: int        # P√°gina no PDF
    bbox: BoundingBox
    confidence: float  # Confian√ßa do matching (0-1)
```

**Fluxo de Extra√ß√£o**:

```
PDF ‚Üí Docling ‚Üí ConversionResult
                     ‚îÇ
                     ‚îú‚îÄ‚îÄ markdown ‚Üí SpanParser ‚Üí ParsedDocument
                     ‚îÇ
                     ‚îî‚îÄ‚îÄ texts[].prov ‚Üí PageSpanExtractor ‚Üí TextLocations
                                                ‚îÇ
                                                ‚ñº
                            map_spans_to_locations() ‚Üí SpanLocations
```

**Uso**:

```python
from docling.document_converter import DocumentConverter
from parsing import SpanParser, PageSpanExtractor

# Converte PDF
converter = DocumentConverter()
result = converter.convert("documento.pdf")

# Extrai page spans
extractor = PageSpanExtractor()
text_locations = extractor.extract_from_docling(result.document)

# Parseia markdown
parser = SpanParser()
parsed_doc = parser.parse(result.document.export_to_markdown())

# Mapeia spans para coordenadas
span_locations = extractor.map_spans_to_locations(parsed_doc, text_locations)

# Resultado
for span_id, loc in span_locations.items():
    print(f"{span_id}: p√°gina {loc.page}, bbox={loc.bbox.to_dict()}")
```

**Integra√ß√£o com ChunkMetadata**:

```python
page_spans = {
    "ART-005": {"page": 2, "l": 100.0, "t": 200.0, "r": 500.0, "b": 220.0},
    "PAR-005-1": {"page": 3, "l": 100.0, "t": 400.0, "r": 500.0, "b": 420.0},
}

metadata = ChunkMetadata(
    schema_version="1.0.0",
    document_hash="abc123",
    page_spans=page_spans,  # Usado para navega√ß√£o visual no frontend
)
```

**Uso no Frontend**:

O frontend pode usar as coordenadas para:
1. Destacar o texto citado no PDF viewer
2. Navegar automaticamente para a p√°gina correta
3. Desenhar bounding box sobre o texto relevante

### 12. Dashboard de Ingest√£o (23/12/2024)

M√≥dulo para coleta e visualiza√ß√£o de m√©tricas do pipeline de ingest√£o.

**M√≥dulo**: `src/dashboard/ingestion_metrics.py`

**M√©tricas Coletadas**:

| Categoria | M√©tricas |
|-----------|----------|
| **Cobertura** | Par√°grafos, incisos, al√≠neas por artigo |
| **Status** | V√°lidos, suspeitos, inv√°lidos |
| **Lat√™ncia** | Tempo por fase, por artigo |
| **Tokens** | Prompt, completion, total |
| **Chunks** | Por tipo (article, paragraph, inciso) |

**Uso**:

```python
from dashboard import MetricsCollector, generate_dashboard_report

# Durante o pipeline
collector = MetricsCollector(ingestion_id="IN-65-2021-001")
collector.set_document_info(
    document_id="IN-65-2021",
    tipo_documento="IN",
    numero="65",
    ano=2021,
)

collector.start_phase("parsing")
# ... parsing ...
collector.end_phase("parsing", items_processed=1)

# Para cada artigo extra√≠do
collector.record_article_metrics(
    article_id="ART-005",
    parser_paragrafos=3,
    llm_paragrafos=3,
    parser_incisos=5,
    llm_incisos=5,
    status="valid",
    tokens_prompt=500,
    tokens_completion=100,
)

# Gera relat√≥rio
report = collector.generate_report()
print(generate_dashboard_report(report))
```

**Exemplo de Sa√≠da**:

```
======================================================================
DASHBOARD DE INGEST√ÉO
======================================================================
Ingestion ID: IN-65-2021-001
Status: completed

----------------------------------------------------------------------
ARTIGOS
----------------------------------------------------------------------
Total: 11
  [OK] Validos: 9 (82%)
  [!!] Suspeitos: 1
  [XX] Invalidos: 1

----------------------------------------------------------------------
COBERTURA
----------------------------------------------------------------------
Par√°grafos: 20/22 (91%)
Incisos: 31/33 (94%)

----------------------------------------------------------------------
CHUNKS GERADOS
----------------------------------------------------------------------
Total: 47
  ARTICLE: 11
  PARAGRAPH: 19
  INCISO: 17

----------------------------------------------------------------------
TOKENS LLM
----------------------------------------------------------------------
Prompt: 5,500
Completion: 1,100
Total: 6,600
Custo estimado (API ref): $0.0066
======================================================================
```

**Classes Principais**:

| Classe | Descri√ß√£o |
|--------|-----------|
| `MetricsCollector` | Coletor principal, registra m√©tricas |
| `ArticleMetrics` | M√©tricas de um artigo individual |
| `DocumentMetrics` | M√©tricas agregadas do documento |
| `PhaseMetrics` | M√©tricas de uma fase do pipeline |
| `IngestionMetrics` | Relat√≥rio completo de ingest√£o |



### 12.1 Dashboard Streamlit - Modos Dev/Prod (25/12/2024)

O dashboard Streamlit suporta dois modos de operacao baseado na variavel RAG_MODE:

**Modos de Operacao**:

| Modo | RAG_MODE | GPU | Comportamento |
|------|----------|-----|---------------|
| **Development** | development (padrao) | 12GB | Lazy loading, modelos sob demanda |
| **Production** | production | 24GB+ | Singleton, modelos na GPU permanentemente |

**Como Alternar**:

- Desenvolvimento: export RAG_MODE=development
- Producao: export RAG_MODE=production

**Comportamento em cada modo**:

| Aspecto | Development | Production |
|---------|-------------|------------|
| Startup | Imediato (~0s) | Lento (~15-20s) |
| Primeira query | Lenta (~30-40s) | Rapida (~10s) |
| VRAM usada | Libera apos uso | Permanente (~8GB) |
| st.cache_resource | Nao pre-carrega | Pre-carrega BGE-M3 + Reranker |

**Indicador Visual na Sidebar**:

O dashboard mostra o modo atual na sidebar:
- **DEVELOPMENT** (azul): Modelos sob demanda
- **PRODUCTION** (verde): Modelos na GPU

### 13. Modulo de Busca Hibrida (22/12/2024)

Modulo reutilizavel para busca 2-stage com BGE-M3 + BGE-Reranker.

**Estrutura**:
```
src/search/
‚îú‚îÄ‚îÄ __init__.py          # Exports publicos
‚îú‚îÄ‚îÄ config.py            # SearchConfig, SearchMode, RerankMode
‚îú‚îÄ‚îÄ models.py            # SearchHit, SearchResult, SearchFilter
‚îî‚îÄ‚îÄ hybrid_searcher.py   # HybridSearcher (classe principal)
```

**Uso Basico**:
```python
from search import HybridSearcher, SearchConfig

# Busca com configuracao padrao (3-way hybrid + reranker)
with HybridSearcher() as searcher:
    result = searcher.search("O que e pesquisa de precos?", top_k=5)

    for hit in result.hits:
        print(f"Art. {hit.article_number}: {hit.final_score:.2f}")
        print(f"  {hit.context_header}")
```

**Configuracoes Pre-definidas**:

| Config | Modo | Reranker | Uso |
|--------|------|----------|-----|
| `SearchConfig.default()` | 3-way hybrid | Cross-encoder | Producao |
| `SearchConfig.fast()` | 2-way hybrid | Nenhum | Baixa latencia |
| `SearchConfig.precise()` | 3-way hybrid | Cross-encoder | Maxima precisao |
| `SearchConfig.dense_only()` | Dense | Nenhum | Debug |

**Busca com Filtros**:
```python
from search import HybridSearcher, SearchFilter

filters = SearchFilter(
    document_type="IN",
    year=2021,
    thesis_types=["definicao", "procedimento"],
)
result = searcher.search("definicoes basicas", filters=filters)
```

**Campos Usados na Busca Hibrida**:

| Vetor | Campo Fonte | Peso | Descricao |
|-------|-------------|------|-----------|
| `dense_vector` | `enriched_text` | 50% | Semantica geral (contexto + texto + perguntas) |
| `sparse_vector` | `enriched_text` | 30% | Termos especificos (learned sparse) |
| `thesis_vector` | `thesis_text` | 20% | Essencia/resumo do artigo |

**Pipeline de Busca 2-Stage**:

```
Query
  ‚îÇ
  ‚ñº
[BGE-M3] ‚Üí Embedding hibrido (dense + sparse)
  ‚îÇ
  ‚ñº
[Stage 1: Milvus Hybrid Search]
  ‚îú‚îÄ Dense (50%) ‚Üí ANN no dense_vector
  ‚îú‚îÄ Sparse (30%) ‚Üí Inverted Index no sparse_vector
  ‚îî‚îÄ Thesis (20%) ‚Üí ANN no thesis_vector
  ‚îÇ
  ‚ñº
[WeightedRanker] ‚Üí Top 20 candidatos
  ‚îÇ
  ‚ñº
[Stage 2: BGE-Reranker Cross-Encoder]
  ‚îî‚îÄ Rerank usando enriched_text
  ‚îÇ
  ‚ñº
Top 5 final (ordenado por relevancia)
```

**Benchmark do Modulo** (22/12/2024 - IN 65/2021 Pesquisa de Precos):

| Query | Top 1 | Rerank Score | Tempo Total |
|-------|-------|--------------|-------------|
| Como fazer pesquisa de precos? | Art. 3 (definicoes) | **0.91** | 43s |
| Prazo de validade dos precos? | Art. 6 (120 dias) | **0.94** | 25s |
| O que e preco estimado? | Art. 2 (definicoes) | **0.78** | 25s |

**Performance**:
- Stage 1 (Milvus): ~70-100ms (apos warmup)
- Stage 2 (Reranker): ~25s (modelo cross-encoder)
- Primeira execucao: ~40s (carrega modelos)

---

## üöÄ Roadmap

### Fase 1 - MVP Extra√ß√£o ‚úÖ (Completo)

- [x] Defini√ß√£o de stack
- [x] Documenta√ß√£o de decis√µes
- [x] Setup projeto Python
- [x] Teste Docling com PDF de lei
- [x] An√°lise da estrutura extra√≠da
- [x] Benchmark de modelos LLM (8 modelos testados)
- [x] Sele√ß√£o do modelo: **qwen3:8b**
- [x] Implementa√ß√£o do Extractor com Pydantic
- [x] Compara√ß√£o: Extractor vs LangGraph vs H√≠brido
- [x] API estilo LlamaExtract (open-source)
- [x] Valida√ß√£o Pydantic integrada

### Fase 2 - Chunking Agentico ‚úÖ (Completo)

- [x] Implementacao do LawChunker (chunk_models.py, law_chunker.py)
- [x] Prompts de enriquecimento (enrichment_prompts.py)
- [x] Cliente vLLM (vllm_client.py)
- [x] Benchmark 4B vs 8B para enriquecimento
- [x] Decisao: 4B para enriquecimento, 8B para extracao
- [x] Integracao completa LawChunker + vLLM + BGE-M3
- [x] Pipeline run_pipeline.py funcional

### Fase 3 - Embeddings + Storage ‚úÖ (Completo)

- [x] Setup vLLM em Docker (producao)
- [x] Download modelos AWQ no volume Docker
- [x] Configurar BGE-M3 (embeddings dense + sparse)
- [x] Configurar bge-reranker-v2-m3 (reranking)
- [x] Schema Milvus (leis_v2 com 30 campos)
- [x] Pipeline de indexacao (run_pipeline.py)
- [x] Busca hibrida (dense + sparse + thesis = 3 vetores)
- [x] 2-Stage retrieval (BGE-M3 + Reranker)
- [x] ColBERT Reranker implementado (alternativa)
- [x] Benchmark ColBERT vs Cross-Encoder (80% concordancia)
- [x] Correcao: usar enriched_text nos embeddings
- [x] Correcao: usar 3 vetores na busca hibrida
- [x] Modulo de busca reutilizavel (`src/search/`)

### Fase 4 - RAG Completo ‚úÖ (Arquitetura Span-Based Completa)

- [x] Modulo de busca hibrida (HybridSearcher)
- [x] Arquitetura Span-Based (SpanParser, ArticleOrchestrator)
- [x] Parent-child retrieval (ChunkMaterializer)
- [x] Schema enum din√¢mico por artigo (previne alucina√ß√£o)
- [x] Retry focado por janela (PAR ou INC)
- [x] Metadados de proveni√™ncia (schema_version, document_hash)
- [x] Answer-JSON estruturado para frontend
- [x] Page spans (coordenadas PDF para cita√ß√µes visuais)
- [x] Dashboard de ingest√£o (m√©tricas de cobertura)
- [x] Schema Milvus v3 (leis_v3) com parent-child
- [x] Migra√ß√£o leis_v2 ‚Üí leis_v3
- [x] Pipeline v3 (run_pipeline_v3.py)
- [x] Busca h√≠brida testada (RRF + Weighted Ranker)
- [x] IN 65/2021 indexada (47 chunks, 100% cobertura)
- [ ] API de busca com FastAPI
- [ ] Integra√ß√£o retrieval + generation (vLLM)
- [ ] Prompts especializados para resposta juridica
- [ ] Avalia√ß√£o de qualidade (RAGAS ou similar)

### Fase 5 - Interface

- [ ] Setup Next.js
- [ ] Interface de upload
- [ ] Interface de busca
- [ ] Dashboard de monitoramento

---

## üìù Notas de Desenvolvimento

### 21/12/2024

**Manha - Extracao**:
- Projeto iniciado
- Stack definida com foco em licencas Apache 2.0/MIT
- Decisao de usar JSON estruturado ao inves de Markdown puro
- Estrutura do projeto criada em `extracao/`
- Ambiente virtual configurado com Docling instalado
- **Teste Docling**: Extracao da IN SEGES 58/2022 bem-sucedida (3.5s)
- **Benchmark completo**: 8 modelos Qwen testados
- **Selecao**: qwen3:8b (94% qualidade, unico com alineas)
- **Comparacao de abordagens**: Extractor (100%) vs LangGraph (30%) vs Hibrido (100%)
- **Decisao**: Extractor Simples como motor principal
- **API criada**: Estilo LlamaExtract, 100% open-source

**Tarde - Chunking e Enriquecimento**:
- Implementado LawChunker (chunking hierarquico por artigo)
- Criados prompts de enriquecimento (Contextual Retrieval da Anthropic)
- Implementado cliente vLLM (API OpenAI-compatible)
- Setup vLLM em Docker com modelos AWQ
- Download Qwen 3 4B-AWQ e 8B-AWQ no volume Docker
- **Benchmark enriquecimento**: 4B vs 8B testados
- **Resultado**: 4B tem mesma qualidade, porem 2x mais rapido (7.4s vs 14.5s/chunk)
- **Decisao**: 4B para enriquecimento, 8B para extracao
- **Estrategia de roteamento**: Pipeline sequencial (trocar modelo entre fases)

**Arquivos criados**:
```
src/chunking/
  chunk_models.py      # LegalChunk dataclass
  law_chunker.py       # LawChunker pipeline
  enrichment_prompts.py # Prompts Contextual Retrieval
src/llm/
  vllm_client.py       # VLLMClient, LLMConfig
src/embeddings/
  bge_m3.py           # BGEM3Embedder (a integrar)
src/milvus/
  schema.py           # leis_v2 collection schema
tests/
  test_chunking.py    # Teste de chunking
  test_enrichment.py  # Benchmark de enriquecimento
```

### 22/12/2024

**Manha - Embeddings e Retrieval**:
- Integrado BGE-M3 com pipeline (dense 1024d + sparse)
- Criada collection `leis_v2` no Milvus (30 campos, 6 indices)
- Pipeline completo: JSON ‚Üí Chunks ‚Üí LLM ‚Üí Embeddings ‚Üí Milvus
- Testada busca hibrida (dense + sparse + RRF/Weighted)
- **2-Stage Retrieval**: BGE-M3 + BGE-Reranker funcionando

**Tarde - ColBERT e Correcoes**:
- Implementado ColBERT Reranker (`src/reranker/colbert_reranker.py`)
- **Benchmark ColBERT vs Cross-Encoder**: 80% concordancia, Cross-Encoder tem scores maiores
- **Decisao**: Cross-Encoder como principal, ColBERT como alternativa
- **Bug encontrado**: `enriched_text` nao estava sendo usado nos embeddings!
- **Correcao**: `law_chunker.py` agora usa `enriched_text` para `dense_vector`
- **Correcao**: Busca hibrida agora usa 3 vetores (dense + sparse + thesis)
- **Correcao**: Reranking agora usa `enriched_text` em vez de `text`

**Arquivos criados/modificados**:
```
src/reranker/
  __init__.py          # Modulo de reranking
  colbert_reranker.py  # ColBERT MaxSim reranker
src/embeddings/
  bge_reranker.py      # BGE-Reranker Cross-Encoder
src/chunking/
  law_chunker.py       # CORRIGIDO: usar enriched_text
tests/
  test_2stage_retrieval.py      # ATUALIZADO: 3 vetores + enriched_text
  test_colbert_vs_crossencoder.py # Benchmark comparativo
scripts/
  init_milvus.py       # Inicializa collection leis_v2
  run_pipeline.py      # Pipeline completo
```

**Tarde - Modulo de Busca Reutilizavel**:
- Criado modulo `src/search/` com API limpa para busca
- `HybridSearcher`: classe principal com busca 2-stage
- `SearchConfig`: configuracoes pre-definidas (default, fast, precise)
- `SearchHit`, `SearchResult`: dataclasses para resultados
- `SearchFilter`: filtros por tipo_documento, ano, thesis_type
- Testado com IN 65/2021 (pesquisa de precos)
- **Resultado**: Reranker scores 0.78-0.94 para queries relevantes
- **Descoberta**: Collection `leis_v2` contem IN 65, nao IN 58

**Arquivos criados**:
```
src/search/
  __init__.py          # Exports publicos
  config.py            # SearchConfig, SearchMode, RerankMode
  models.py            # SearchHit, SearchResult, SearchFilter
  hybrid_searcher.py   # HybridSearcher (classe principal)
tests/
  test_search_module.py # Teste completo do modulo
```

### 23/12/2024

**Manh√£ - Arquitetura Span-Based**:
- Implementada arquitetura de extra√ß√£o baseada em spans
- Criado `SpanParser` que parseia Markdown para spans hier√°rquicos
- Criado `ArticleOrchestrator` que extrai hierarquia por artigo
- Schema enum din√¢mico: IDs permitidos passados como enum no JSON Schema
- Curto-circuito: artigos sem filhos n√£o chamam LLM (economia de tokens)
- Retry focado por janela: retry para PAR ou INC, n√£o ambos juntos
- Valida√ß√£o de parent consistency (INC-005-I_2 ‚Üí parent=PAR-005-2)
- **Resultado**: 100% de acur√°cia na IN 65/2021 (11/11 artigos v√°lidos)

**Tarde - Parent-Child e Answer-JSON**:
- Criado `ChunkMaterializer` para parent-child retrieval
- Chunks pai (ARTICLE) + chunks filhos (PARAGRAPH/INCISO)
- Metadados de proveni√™ncia: schema_version, extractor_version, document_hash
- Criado m√≥dulo `rag/` com Answer-JSON para frontend
- Formato estruturado: answer, citations, confidence, sources, metadata
- C√°lculo de confian√ßa baseado em relev√¢ncia ponderada
- **Resultado**: 47 chunks materializados (11 ART + 19 PAR + 17 INC)

**Arquivos criados**:
```
src/parsing/
  span_parser.py              # SpanParser
  span_models.py              # Span, SpanType, ParsedDocument
  span_extraction_models.py   # ArticleSpans schema
  article_orchestrator.py     # ArticleOrchestrator
  __init__.py                 # Exports
src/chunking/
  chunk_materializer.py       # ChunkMaterializer, MaterializedChunk
  __init__.py                 # ATUALIZADO: novos exports
src/rag/
  __init__.py                 # M√≥dulo RAG
  answer_models.py            # AnswerResponse, Citation, Source
tests/
  test_span_parser.py         # Teste do SpanParser
  test_article_orchestrator.py # Teste do orchestrator
  test_chunk_materializer.py  # Teste parent-child
```

**Problemas resolvidos**:
- JSON truncado com max_tokens=4096 ‚Üí reduzido para 512 (suficiente para IDs)
- KeyError 'article_id' no prompt ‚Üí adicionado ao format()
- Campo `llm_children_count` obsoleto ‚Üí atualizado para campos por tipo

**Noite - Page Spans para Cita√ß√µes Visuais**:
- Criado m√≥dulo `page_spans.py` para extrair coordenadas PDF
- `PageSpanExtractor`: extrai bounding boxes do Docling
- `BoundingBox`: estrutura com l/t/r/b + page + coord_origin
- `SpanLocation`: mapeia span_id para localiza√ß√£o no PDF
- Integra√ß√£o com `ChunkMetadata.page_spans`
- **Resultado**: 100% de mapeamento em testes com 4 spans

**Arquivos criados**:
```
src/parsing/
  page_spans.py           # PageSpanExtractor, BoundingBox, SpanLocation
tests/
  test_page_spans.py      # Testes de mapeamento e merge
```

**Dashboard de Ingest√£o**:
- Criado m√≥dulo `dashboard/` para m√©tricas de ingest√£o
- `MetricsCollector`: coleta m√©tricas durante o pipeline
- `ArticleMetrics`: cobertura por artigo (PAR, INC)
- `DocumentMetrics`: agrega√ß√£o de documento
- `PhaseMetrics`: timing por fase (parsing, extraction)
- `generate_dashboard_report()`: relat√≥rio formatado para terminal
- **Resultado**: Dashboard completo com cobertura, tokens, custo, lat√™ncia

**Arquivos criados**:
```
src/dashboard/
  __init__.py              # Exports p√∫blicos
  ingestion_metrics.py     # MetricsCollector, m√©tricas
tests/
  test_dashboard.py        # Testes de coleta e relat√≥rio
```

### 23/12/2024

**Madrugada - Schema Milvus v3 e Migra√ß√£o**:
- Criado schema `leis_v3` com campos parent-child e proveni√™ncia
- Novos campos: `parent_chunk_id`, `span_id`, `device_type`, `chunk_level`
- Campos de proveni√™ncia: `schema_version`, `extractor_version`, `ingestion_timestamp`, `document_hash`
- Campos page spans: `page`, `bbox_left`, `bbox_top`, `bbox_right`, `bbox_bottom`
- Script de migra√ß√£o `migrate_to_v3.py`: dropa leis_v2, cria leis_v3
- **Resultado**: Collection leis_v3 com 30 campos e 8 √≠ndices

**Arquivos criados**:
```
src/milvus/
  schema_v3.py          # Schema v3 com parent-child
  __init__.py           # ATUALIZADO: exports v3
scripts/
  migrate_to_v3.py      # Migra√ß√£o leis_v2 ‚Üí leis_v3
```

**Pipeline v3 - Span-Based + Milvus**:
- Criado `run_pipeline_v3.py`: pipeline completo com nova arquitetura
- Fluxo: SpanParser ‚Üí ArticleOrchestrator ‚Üí ChunkMaterializer ‚Üí BGE-M3 ‚Üí Milvus
- Integra√ß√£o com MetricsCollector para dashboard
- **Bug fix**: `chunk_level.value` retornava int, alterado para `.name.lower()` (string)
- **Bug fix**: `embedder.encode()` n√£o retorna sparse, alterado para `encode_hybrid()`
- **Resultado**: IN 65/2021 inserida com sucesso (47 chunks, 30.02s total)

**Arquivos criados**:
```
scripts/
  run_pipeline_v3.py    # Pipeline v3 completo
```

**Teste de Busca H√≠brida (Dense + Sparse)**:
- Testada busca h√≠brida no Milvus com RRF e Weighted Ranker
- Query: "Como fazer pesquisa de pre√ßos em contrata√ß√µes p√∫blicas?"
- Compara√ß√£o de rankings entre m√©todos

| M√©todo | Top 1 | Top 2 | Top 3 | Score Top 1 |
|--------|-------|-------|-------|-------------|
| Dense Only | ART-005 | INC-005-IV | ART-001 | 0.6512 |
| Sparse Only | INC-005-IV | ART-003 | ART-004 | 0.1020 |
| **RRF Hybrid** | INC-005-IV | ART-005 | ART-003 | 0.0325 |
| Weighted (0.7/0.3) | ART-005 | INC-005-IV | ART-001 | 0.7372 |

**Observa√ß√µes**:
- Overlap dense/sparse: 4/5 (80% de concord√¢ncia)
- RRF promove INC-005-IV para Top 1 (combina√ß√£o sem√¢ntica + lexical)
- Weighted mant√©m ranking similar ao dense (pondera√ß√£o 70/30)
- Sparse scores s√£o menores mas capturam termos exatos ("pesquisa", "pre√ßos")

**Resultados da Ingest√£o IN 65/2021**:
```
Pipeline v3 - Status: completed
Tempo total: 30.02s

Fases:
- Load: 0.00s
- Parsing: 0.00s (57 spans)
- Extraction: 12.08s (11 artigos v√°lidos)
- Materialization: 0.00s (47 chunks)
- Embedding: 16.28s (47 embeddings BGE-M3)
- Indexing: 1.65s (47 inseridos no Milvus)

Cobertura: 100%
- Par√°grafos: 19/19
- Incisos: 17/17

Chunks por tipo:
- ARTICLE: 11
- PARAGRAPH: 19
- INCISO: 17
```

**Afina√ß√µes Finais - Contextual Retriever**:
- Criado m√≥dulo `ContextualRetriever` com parent-child + MMR
- Query Router autom√°tico: Weighted (padr√£o) vs RRF (dispositivo espec√≠fico)
- MMR (Maximal Marginal Relevance) para diversidade de irm√£os
- Cap de expans√£o: max 1 pai + 4 irm√£os relevantes
- `CitationValidator`: valida citations ‚äÜ context_used

**Arquivos criados**:
```
src/search/
  contextual_retriever.py   # ContextualRetriever, CitationValidator
  __init__.py               # ATUALIZADO: novos exports
scripts/
  benchmark_retrieval.py    # Benchmark de estrat√©gias
```

**Query Router - Detec√ß√£o Autom√°tica**:
```python
# Padr√µes que ativam RRF (dispositivo espec√≠fico)
DEVICE_PATTERNS = [
    r'\bart\.?\s*\d+',      # art. 5, art 10
    r'¬ß\s*\d+',             # ¬ß 1¬∫
    r'\binciso\b',          # inciso
    r'\bal[i√≠]nea\b',       # al√≠nea
    r'\b[IVX]+\s*[-‚Äì]',     # I -, II -
]

# Queries amplas ‚Üí Weighted (0.7 dense + 0.3 sparse)
# Queries espec√≠ficas ‚Üí RRF (Reciprocal Rank Fusion)
```

**Fluxo do ContextualRetriever**:
```
Query ‚Üí Detecta Estrat√©gia ‚Üí Busca H√≠brida (Top-K)
                                    ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                               ‚ñº
            Expande para Pais              Seleciona Irm√£os (MMR)
            (max 1 artigo)                 (max 4, Œª=0.7)
                    ‚îÇ                               ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚ñº
                           Ordena Hierarquicamente
                           (pai ‚Üí filhos ordenados)
                                    ‚îÇ
                                    ‚ñº
                           Monta Contexto + Cita√ß√µes
```

**Benchmark Final - Contextual vs Simples**:

| Query | Simples (Top-5) | Contextual (MMR) | Ganho |
|-------|-----------------|------------------|-------|
| Fornecedores? | 5 chunks | 9 chunks (5+4 MMR) | +4 irm√£os |
| Prazo resposta? | 5 chunks | 9 chunks (5+4 MMR) | +4 irm√£os |
| Cota√ß√£o formal? | 5 chunks | 9 chunks (5+4 MMR) | +4 irm√£os |

O MMR garante diversidade: n√£o retorna 5 incisos similares, mas mix de PAR + INC.

**Tarde - Enriquecimento LLM + HyDE (Contextual Retrieval)**:

Implementamos o sistema completo de enriquecimento de chunks com LLM e HyDE para query expansion.

**M√≥dulo ChunkEnricher** (`src/enrichment/`):
- Enriquece chunks com contexto, tese e perguntas sint√©ticas
- Usa prompts de `enrichment_prompts.py` (Anthropic Contextual Retrieval)
- Campos preenchidos: `context_header`, `thesis_text`, `thesis_type`, `synthetic_questions`
- Monta `enriched_text` para embedding: `[CONTEXTO: ...] + texto + [PERGUNTAS: ...]`

**Arquivos criados**:
```
src/enrichment/
  __init__.py           # Exports
  chunk_enricher.py     # ChunkEnricher, EnrichmentResult
```

**Integra√ß√£o no Pipeline v3**:
- Nova fase 4.5: Enriquecimento (entre materializa√ß√£o e embedding)
- Processa em batches de 5 chunks
- Usa `Qwen/Qwen3-8B-AWQ` (mesmo modelo √∫nico)
- Tempo: ~5s/chunk (233s para IN 65 com 47 chunks)

**HyDE - Hypothetical Document Embeddings** (`src/search/`):
- T√©cnica de query expansion: gera documentos hipot√©ticos com LLM
- Combina embeddings da query + docs hipot√©ticos (40%/60%)
- Melhora recall para queries amb√≠guas ou curtas
- Toggle: `SearchConfig.use_hyde = True/False`

**Arquivos criados**:
```
src/search/
  hyde_expander.py      # HyDEExpander, HyDEResult
```

**Integra√ß√£o no HybridSearcher**:
- Propriedade `hyde_expander` com lazy loading
- Usa HyDE quando `config.use_hyde = True`
- Gera 3 documentos hipot√©ticos por query
- Overhead: +15-20s por query (gera√ß√£o LLM)

**Benchmark HyDE** (23/12/2024):

| Query | Sem HyDE | Com HyDE | Diferen√ßa |
|-------|----------|----------|-----------|
| "pesquisa de pre√ßos" | ART-005, ART-003, ART-004 | ART-005, ART-003, ART-004 | = (query espec√≠fica) |
| "fornecedores e cota√ß√µes" | PAR-007-5, INC-005-IV | PAR-007-5, INC-082-VII, INC-023-IV | +3 novos resultados Lei 14.133 |

**Conclus√£o HyDE**:
- √ötil para queries curtas/amb√≠guas
- Overhead de +15-20s n√£o justifica para queries espec√≠ficas
- Recomendado: desabilitado por padr√£o, habilitado para queries complexas

**Resultados IN 65/2021 com Enriquecimento**:
```
Pipeline v3 - Status: completed
Tempo total: 279.67s

Fases:
- Load: 0.00s
- Parsing: 0.00s (57 spans)
- Extraction: 11.49s (11 artigos v√°lidos)
- Materialization: 0.00s (47 chunks)
- Enrichment: 233.05s (47 chunks, 0 erros)
- Embedding: 33.37s (47 embeddings BGE-M3)
- Indexing: 1.76s

Cobertura: 100%
Campos preenchidos: context_header, thesis_text, thesis_type, synthetic_questions, enriched_text
```

**Resultados Lei 14.133/2021** (sem enriquecimento por timeout):
```
Pipeline v3 - Status: completed
Tempo total: 945.84s

Fases:
- Extraction: 294.36s (191/204 v√°lidos, 94%)
- Materialization: 1265 chunks
- Enrichment: TIMEOUT ap√≥s 3 batches
- Embedding: 240.43s (1265 embeddings)
- Indexing: 2.99s

Chunks no Milvus: 1312 total (47 IN 65 + 1265 Lei 14.133)
```

**Corre√ß√£o de Bug - Nome do Modelo vLLM**:
- LLMConfig usava `Qwen/Qwen3-8B` mas container tem `Qwen/Qwen3-8B-AWQ`
- Corrigido: `for_enrichment()` e `for_extraction()` agora usam `-AWQ`
- Arquivo: `src/llm/vllm_client.py`

---

## üß™ Benchmark de Modelos LLM (21/12/2024)

### Metodologia

- **Documento teste**: Instru√ß√£o Normativa SEGES N¬∫ 58/2022 (19 artigos, 4 cap√≠tulos)
- **Tarefa**: Converter Markdown (extra√≠do pelo Docling) para JSON estruturado
- **Crit√©rios**: Extra√ß√£o de items (incisos), paragraphs, sub_items (al√≠neas), t√≠tulos
- **Hardware**: NVIDIA RTX 4070 12GB VRAM

### Resultados Completos

| Modelo            | Disco     | GPU Load     | items        | paragraphs | sub_items       | T√≠tulos | Qualidade          |
| ----------------- | --------- | ------------ | ------------ | ---------- | --------------- | ------- | ------------------ |
| qwen2.5:7b        | 4.7GB     | 100% GPU     | ‚ùå Mistura ¬ß | ‚ùå         | ‚ùå              | ‚ùå      | ‚≠ê‚≠ê‚≠ê 75%         |
| qwen2.5-coder:7b  | 4.7GB     | 100% GPU     | ‚ùå           | ‚ùå         | ‚ùå              | ‚ùå      | ‚≠ê‚≠ê 60%           |
| qwen2.5-coder:14b | 9GB       | 94% GPU      | ‚úÖ           | ‚úÖ         | ‚ùå              | ‚ùå      | ‚≠ê‚≠ê‚≠ê‚≠ê 93%       |
| qwen3:4b          | 2.5GB     | 100% GPU     | ‚úÖ           | ‚úÖ         | ‚ùå              | ‚ùå      | ‚≠ê‚≠ê‚≠ê‚≠ê 90%       |
| **qwen3:8b** ‚≠ê   | **4.9GB** | **100% GPU** | ‚úÖ           | ‚úÖ         | **‚úÖ Al√≠neas!** | ‚úÖ      | **‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 94%** |
| qwen3:14b         | 9GB       | 74%/26% CPU  | ‚úÖ           | ‚úÖ         | ‚ö†Ô∏è Parcial      | ‚ùå      | ‚≠ê‚≠ê‚≠ê‚≠ê 92%       |
| qwen3-coder:30b   | 18GB      | Local MoE    | ‚úÖ           | ‚úÖ         | ‚úÖ (vazio)      | ‚úÖ      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 97%     |
| qwen3-coder:480b  | Cloud     | Cloud        | ‚úÖ           | ‚úÖ         | ‚úÖ (vazio)      | ‚úÖ      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 98%     |

### An√°lise

**Observa√ß√µes Importantes**:

1. **qwen3:8b** foi o √∫nico modelo local que extraiu corretamente **al√≠neas (sub_items: a, b, c, d)**
2. Modelos com **offloading CPU** (qwen3:14b) tiveram qualidade inferior aos que cabem 100% na GPU
3. Fam√≠lia **Qwen 3** √© significativamente melhor que **Qwen 2.5** para extra√ß√£o estruturada
4. Modelos **coder** n√£o mostraram vantagem significativa para esta tarefa espec√≠fica
5. O modelo cloud (480b) √© apenas 4% melhor que o qwen3:8b local

**Decis√£o Final**: **qwen3:8b**

| Crit√©rio         | qwen3:8b         |
| ---------------- | ---------------- |
| Tamanho em disco | 4.9GB            |
| Uso de VRAM      | ~10GB (100% GPU) |
| Contexto m√°ximo  | 256K tokens      |
| Qualidade JSON   | 94%              |
| Extra√ß√£o al√≠neas | ‚úÖ √önico local   |
| Custo            | Gratuito (local) |
| Licen√ßa          | Apache 2.0       |

### Modelos Mantidos no Sistema

```
qwen3:8b                 - 5.2GB  - Produ√ß√£o
qwen3-coder:480b-cloud   - Cloud  - Backup/Compara√ß√£o
```

### Modelos Removidos

- qwen2.5:7b (75% qualidade)
- qwen2.5-coder:7b (60% qualidade - muito incompleto)
- qwen2.5-coder:14b (93% mas precisa de contexto reduzido)
- qwen3:4b (90% - substitu√≠do pelo 8b)
- qwen3:14b (92% - offloading prejudica qualidade)
- qwen3-coder:30b (97% - n√£o cabe na GPU 12GB)

---

## üß™ Benchmark de Abordagens de Extra√ß√£o (21/12/2024)

### Metodologia

- **Documento teste**: Instru√ß√£o Normativa SEGES N¬∫ 58/2022 (19 artigos, 4 cap√≠tulos)
- **Tarefa**: Extrair JSON estruturado completo com valida√ß√£o
- **Modelo**: qwen3:8b

### Resultados

| Abordagem             |  Score   | Cap√≠tulos | Schema  | Al√≠neas | Metadados |
| --------------------- | :------: | :-------: | :-----: | :-----: | :-------: |
| **Extractor Simples** | **100%** |   4 ‚úÖ    |  OK ‚úÖ  | SIM ‚úÖ  |  SIM ‚úÖ   |
| LangGraph Pipeline    |   30%    |   2 ‚ùå    | ERRO ‚ùå | N√ÉO ‚ùå  |  N√ÉO ‚ùå   |
| **H√≠brido**           | **100%** |   4 ‚úÖ    |  OK ‚úÖ  | SIM ‚úÖ  |  SIM ‚úÖ   |

### Conclus√£o

O **Pydantic Schema** √© o diferencial. Quando o LLM recebe o JSON Schema completo do Pydantic, ele sabe exatamente o que gerar.

---

## üß™ Benchmark de Enriquecimento: 4B vs 8B (21/12/2024)

### Metodologia

- **Documento teste**: IN SEGES 58/2022 (3 artigos selecionados: Art. 3, 6, 14)
- **Tarefa**: Gerar context_header, thesis_text, thesis_type, synthetic_questions
- **Runtime**: vLLM 0.13 com quantizacao AWQ
- **Hardware**: NVIDIA RTX 4070 12GB VRAM

### Resultados Comparativos

| Metrica | **4B-AWQ** | **8B-AWQ** | Diferenca |
|---------|-----------|-----------|-----------|
| Taxa de sucesso | 100% (3/3) | 100% (3/3) | = |
| Acuracia thesis_type | 66.7% (2/3) | 66.7% (2/3) | = |
| Tempo total | **22.20s** | 43.46s | **-49%** |
| Tempo medio/chunk | **7.40s** | 14.49s | **-49%** |

### Tempo por Artigo

| Artigo | Tipo Esperado | 4B (tempo) | 8B (tempo) | Speedup | Acerto |
|--------|---------------|-----------|-----------|---------|--------|
| Art. 3 | definicao | 9.25s | 16.81s | **1.8x** | OK |
| Art. 6 | procedimento | 6.43s | 12.76s | **2.0x** | ERRO* |
| Art. 14 | excecao | 6.52s | 13.89s | **2.1x** | OK |

*O erro no Art. 6 ocorreu em ambos os modelos - e problema do prompt, nao do modelo.

### Qualidade das Saidas (4B)

**Art. 3 (Definicoes)**:
```
context_header: Este artigo da IN 58/2022 define os conceitos basicos para
                elaboracao de ETP no ambito federal

thesis_text: Estabelece definicoes de termos tecnicos relacionados ao
             planejamento de contratacoes publicas, incluindo ETP, sistema
             digital de gestao e tipos de contratacoes correlatas ou interdependentes

synthetic_questions:
- Qual e a definicao de ETP segundo a IN 58/2022?
- Quais sao as caracteristicas do Sistema ETP Digital?
- O que caracteriza contratacoes correlatas?
```

### Decisao Final

| Criterio | 4B-AWQ | 8B-AWQ | Vencedor |
|----------|--------|--------|----------|
| Qualidade | 100% | 100% | Empate |
| Velocidade | 7.4s/chunk | 14.5s/chunk | **4B** |
| VRAM | 2.5GB | 5.7GB | **4B** |
| Batch potencial | Maior | Menor | **4B** |

**Conclusao**: O **Qwen 3 4B-AWQ** e a escolha certa para enriquecimento:
- Mesma qualidade que o 8B
- 2x mais rapido
- Metade da VRAM (permite batch maior no futuro)

---

## üîç Arquitetura de Validacao (Pos-MVP)

> **Status**: Planejado - Implementar apos MVP funcional

### Problema

A extracao tem duas etapas que precisam de validacao:

1. **PDF ‚Üí Markdown (Docling)**: Como saber se o Docling extraiu corretamente?
2. **Markdown ‚Üí JSON (Qwen3)**: Como saber se o LLM estruturou corretamente?

### Decisao: Validacao Assincrona (Nao-Bloqueante)

Em vez de validar sincronamente (bloqueando o pipeline), a validacao roda em **paralelo**:

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ      Processo Principal      ‚îÇ
                    ‚îÇ    (nao bloqueia, rapido)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
PDF ‚Üí Docling ‚Üí Markdown ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Qwen3 ‚Üí JSON ‚Üí Milvus
                                  ‚îÇ
                                  ‚îÇ (fork assincrono)
                                  ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    Processo de Validacao     ‚îÇ
                    ‚îÇ   (paralelo, pode demorar)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Alerta para Humano         ‚îÇ
                    ‚îÇ   (so se score < threshold)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Beneficios

| Beneficio | Descricao |
|-----------|-----------|
| **Desacoplamento** | Processo principal nao espera validacao |
| **Escala independente** | Workers de extracao e validacao escalam separadamente |
| **Human-in-the-loop inteligente** | Humano so e acionado quando necessario |
| **Retry sem reprocessar** | Se validacao falhar, pode re-validar sem re-extrair |

### Estados do Documento

```
PROCESSANDO ‚Üí EXTRAIDO ‚Üí INDEXADO
                 ‚îÇ
                 ‚îî‚îÄ‚îÄ‚Üí VALIDANDO ‚Üí VALIDADO ‚úì
                           ‚îÇ
                           ‚îî‚îÄ‚îÄ‚Üí SUSPEITO ‚Üí REVISAO_HUMANA ‚Üí CORRIGIDO
```

O documento pode estar **indexado e buscavel** mesmo enquanto validacao roda.

### Validacao 1: PDF ‚Üí Markdown (Heuristicas)

Usar **PyMuPDF** para extrair texto bruto do PDF e comparar contagens com Markdown do Docling:

| Elemento | Contagem PDF | Contagem Markdown | Status |
|----------|--------------|-------------------|--------|
| Artigos (Art.) | 19 | 19 | ‚úÖ |
| Paragrafos (¬ß) | 10 | 10 | ‚úÖ |
| Incisos (I, II) | 25 | 25 | ‚úÖ |
| Capitulos | 4 | 4 | ‚úÖ |

**Alertar humano se**: Discrepancia > 5% ou elementos estruturais faltando.

### Validacao 2: Markdown ‚Üí JSON (Ja Implementado)

Modulo `extraction_utils.py` com:

- **DoclingValidator**: Conta elementos no Markdown
- **ExtractionValidator**: Compara JSON vs Markdown
- **AutoFixer**: Corrige erros conhecidos automaticamente

### Threshold de Alerta

| Score | Acao |
|-------|------|
| >= 98% | Log apenas, sem alerta |
| 95-98% | Warning, revisao opcional |
| < 95% | Alerta para humano |
| Erros estruturais | Alerta imediato |

### Implementacao Futura

**Fase 1 (Simples)**:
- Fila em memoria ou SQLite
- Worker separado (thread/processo)
- Alerta por log/arquivo

**Fase 2 (Producao)**:
- Redis/RabbitMQ como fila
- Celery para workers
- Dashboard para humano revisar

**Fase 3 (Escala)**:
- Evento no MinIO quando PDF e processado
- Workers distribuidos
- Metricas e observabilidade

### Amostragem para Calibracao

Mesmo com score 100%, **10% dos documentos** vao para revisao aleatoria para:
- Calibrar confianca no sistema
- Detectar erros sistematicos
- Melhorar heuristicas ao longo do tempo

---

## üõ°Ô∏è Corre√ß√µes Anti-Alucina√ß√£o no Extrator (22/12/2024)

### Problema Identificado

Durante testes com a IN 65/2021 (pesquisa de pre√ßos), descobrimos que o LLM estava **inventando artigos** que n√£o existiam no documento original. O problema tinha duas causas:

1. **Cap√≠tulo Fantasma**: O m√©todo `_split_by_chapters` criava um cap√≠tulo "DISPOSI√á√ïES INICIAIS" para conte√∫do antes do primeiro CAP√çTULO real (que era apenas metadados como ementa e data). O LLM ent√£o tentava extrair artigos desse conte√∫do e inventava artigos.

2. **Refer√™ncias a Outras Leis**: O texto do documento continha refer√™ncias a artigos de outras leis (ex: "Art. 75 da Lei 14.133"). A valida√ß√£o p√≥s-extra√ß√£o capturava essas refer√™ncias como artigos v√°lidos.

### Corre√ß√µes Implementadas

| Corre√ß√£o | Arquivo | Descri√ß√£o |
|----------|---------|-----------|
| Ignorar conte√∫do pr√©-cap√≠tulo | `extractor.py:_split_by_chapters` | S√≥ processa conte√∫do AP√ìS o primeiro CAP√çTULO real |
| Valida√ß√£o pr√©-extra√ß√£o | `extractor.py:_extract_chapter` | Verifica se cap√≠tulo tem artigos antes de chamar LLM |
| Instru√ß√£o anti-alucina√ß√£o | `extractor.py:_extract_chapter` | Prompt inclui lista de artigos esperados e pro√≠be inven√ß√£o |
| Valida√ß√£o p√≥s-extra√ß√£o | `extractor.py:_validate_extracted_chapter` | Remove artigos que n√£o existem no markdown original |
| Distin√ß√£o de refer√™ncias | `extractor.py:_validate_extracted_chapter` | Ignora "Art. N da Lei X" (refer√™ncias a outras leis) |

### Detalhes T√©cnicos

**`_split_by_chapters` (antes)**:
```python
current_title = "DISPOSI√á√ïES INICIAIS"  # Criava cap√≠tulo fantasma
```

**`_split_by_chapters` (depois)**:
```python
current_title = None  # Ignora conte√∫do antes do primeiro CAP√çTULO
found_first_chapter = False
# S√≥ adiciona conte√∫do ap√≥s encontrar primeiro CAP√çTULO real
```

**Valida√ß√£o pr√©-extra√ß√£o**:
```python
# Lista artigos que realmente existem no texto
expected_articles = sorted(set(int(a) for a in articles_in_content))
# Prompt informa: "Extraia APENAS os artigos: Art. 1, 2, 3..."
# Prompt pro√≠be: "NAO INVENTE artigos que nao estao no texto"
```

**Valida√ß√£o p√≥s-extra√ß√£o**:
```python
# Pattern que ignora refer√™ncias a outras leis
r'(?:^|\n)\s*Art\.?\s*(\d+)[¬∞¬∫o]?(?:\s|\.|\s*[-‚Äì‚Äî])'  # Art. no in√≠cio de linha
# Exclui: "art. N da Lei", "art. N do Decreto"
```

### Resultado

| M√©trica | Antes | Depois |
|---------|-------|--------|
| IN 65/2021 | 21 artigos (10 inventados) | 11 artigos (correto) |
| Alucina√ß√£o | 47% falsos | 0% falsos |
| Score qualidade | 52% | 100% |

### Arquivos Modificados

- `src/extract/extractor.py`: 3 m√©todos corrigidos/adicionados
- `tests/test_extraction_fix.py`: Novo teste de valida√ß√£o

---

## üìä Resumo do Estado Atual (23/12/2024)

### O que est√° funcionando

| Componente | Status | Descri√ß√£o |
|------------|--------|-----------|
| **Docling** | ‚úÖ | Extra√ß√£o PDF ‚Üí Markdown com hierarquia |
| **SpanParser** | ‚úÖ | Markdown ‚Üí Spans determin√≠sticos |
| **ArticleOrchestrator** | ‚úÖ | Extra√ß√£o LLM por artigo com enum din√¢mico |
| **ChunkMaterializer** | ‚úÖ | Parent-child chunks (ART ‚Üí PAR/INC) |
| **ChunkEnricher** | ‚úÖ | Enriquecimento LLM (context, thesis, questions) |
| **BGE-M3** | ‚úÖ | Embeddings dense (1024d) + sparse |
| **Milvus leis_v3** | ‚úÖ | 1312 chunks (IN 65 + Lei 14.133), 30 campos |
| **Busca H√≠brida** | ‚úÖ | Weighted (0.7/0.3) + RRF |
| **HyDEExpander** | ‚úÖ | Query expansion com documentos hipot√©ticos |
| **ContextualRetriever** | ‚úÖ | Parent-child + MMR + Query Router |
| **CitationValidator** | ‚úÖ | Valida citations ‚äÜ context_used |
| **Dashboard** | ‚úÖ | M√©tricas de cobertura e lat√™ncia |

### Arquitetura Implementada

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         PIPELINE DE INGEST√ÉO                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  PDF ‚Üí Docling ‚Üí Markdown ‚Üí SpanParser ‚Üí ArticleOrchestrator (LLM)      ‚îÇ
‚îÇ                                              ‚îÇ                          ‚îÇ
‚îÇ                                              ‚ñº                          ‚îÇ
‚îÇ                                    ChunkMaterializer                    ‚îÇ
‚îÇ                                    (parent-child)                       ‚îÇ
‚îÇ                                              ‚îÇ                          ‚îÇ
‚îÇ                                              ‚ñº                          ‚îÇ
‚îÇ                                    ChunkEnricher (LLM)                  ‚îÇ
‚îÇ                            (context, thesis, questions)                 ‚îÇ
‚îÇ                                              ‚îÇ                          ‚îÇ
‚îÇ                                              ‚ñº                          ‚îÇ
‚îÇ                              BGE-M3 (dense + sparse)                    ‚îÇ
‚îÇ                                              ‚îÇ                          ‚îÇ
‚îÇ                                              ‚ñº                          ‚îÇ
‚îÇ                                    Milvus leis_v3                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         PIPELINE DE RETRIEVAL                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Query ‚Üí [HyDE opcional] ‚Üí Query Router ‚Üí Busca H√≠brida                 ‚îÇ
‚îÇ              ‚îÇ                    ‚îÇ                                     ‚îÇ
‚îÇ              ‚ñº                    ‚ñº                                     ‚îÇ
‚îÇ    Gera docs hipot√©ticos   Detecta padr√µes                              ‚îÇ
‚îÇ    (se use_hyde=True)      (art., ¬ß, inciso)                            ‚îÇ
‚îÇ              ‚îÇ                   ‚îÇ                                       ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
‚îÇ                                  ‚ñº                   ‚ñº                   ‚îÇ
‚îÇ                           Top-K inicial (5)    Weighted/RRF             ‚îÇ
‚îÇ                                  ‚îÇ                                       ‚îÇ
‚îÇ                                  ‚ñº                                       ‚îÇ
‚îÇ                          Expande para Pais (1)                          ‚îÇ
‚îÇ                                  ‚îÇ                                       ‚îÇ
‚îÇ                                  ‚ñº                                       ‚îÇ
‚îÇ                          MMR Irm√£os (4)                                 ‚îÇ
‚îÇ                                  ‚îÇ                                       ‚îÇ
‚îÇ                                  ‚ñº                                       ‚îÇ
‚îÇ                          CitationValidator                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### M√©tricas de Performance

| M√©trica | Valor |
|---------|-------|
| Total chunks Milvus | 1312 (IN 65 + Lei 14.133) |
| IN 65: Cobertura | 100% (par√°grafos e incisos) |
| IN 65: Chunks enriquecidos | 47/47 (100%) |
| Lei 14.133: Cobertura | 99% PAR, 100% INC |
| Lei 14.133: Extra√ß√£o v√°lida | 191/204 (94%) |
| Tempo ingest√£o (IN 65 c/ enriquecimento) | 280s |
| Tempo ingest√£o (Lei 14.133 s/ enriq.) | 946s |

### Arquivos Principais

```
extracao/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ parsing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ span_parser.py           # SpanParser (determin√≠stico)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ span_models.py           # Span, ParsedDocument
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ article_orchestrator.py  # Extra√ß√£o LLM por artigo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page_spans.py            # Coordenadas PDF
‚îÇ   ‚îú‚îÄ‚îÄ chunking/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunk_materializer.py    # Parent-child chunks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunk_models.py          # LegalChunk, ChunkLevel
‚îÇ   ‚îú‚îÄ‚îÄ enrichment/                  # NOVO (23/12/2024)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Exports
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunk_enricher.py        # ChunkEnricher (context, thesis, questions)
‚îÇ   ‚îú‚îÄ‚îÄ search/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hyde_expander.py         # NOVO: HyDE query expansion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contextual_retriever.py  # MMR + Query Router
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_searcher.py       # Busca h√≠brida (HyDE integrado)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py                # SearchConfig (use_hyde toggle)
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vllm_client.py           # VLLMClient (Qwen/Qwen3-8B-AWQ)
‚îÇ   ‚îú‚îÄ‚îÄ milvus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_v3.py             # Schema leis_v3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.py                # Schema legado v2
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bge_m3.py                # BGE-M3 embedder
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ingestion_metrics.py     # M√©tricas de ingest√£o
‚îÇ   ‚îî‚îÄ‚îÄ rag/
‚îÇ       ‚îî‚îÄ‚îÄ answer_models.py         # Answer-JSON
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ run_pipeline_v3.py           # Pipeline c/ enriquecimento (fase 4.5)
‚îÇ   ‚îú‚îÄ‚îÄ migrate_to_v3.py             # Migra√ß√£o Milvus
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_retrieval.py       # Benchmark estrat√©gias
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ test_span_parser.py
    ‚îú‚îÄ‚îÄ test_article_orchestrator.py
    ‚îú‚îÄ‚îÄ test_chunk_materializer.py
    ‚îî‚îÄ‚îÄ test_page_spans.py
```

---

## üéØ Pr√≥ximos Passos (Roadmap)

### Conclu√≠do (23/12/2024)

- [x] **ChunkEnricher**: Enriquecimento LLM (context, thesis, questions)
- [x] **HyDE Query Expansion**: Documentos hipot√©ticos para queries amb√≠guas
- [x] **IN 65 Enriquecida**: 47 chunks com campos preenchidos
- [x] **Lei 14.133 Indexada**: 1265 chunks (sem enriquecimento por timeout)

### Curto Prazo (pr√≥xima sess√£o)

- [ ] **Enriquecer Lei 14.133**: Re-executar pipeline com timeout maior
- [ ] **Grid Search de Pesos**: Testar 0.6/0.4 e 0.8/0.2 para Weighted
- [ ] **Normaliza√ß√£o Sparse**: Lower, stopwords jur√≠dicas, de-accent
- [ ] **Mais documentos**: Indexar IN 58/2022, outras INs

### M√©dio Prazo (API e Integra√ß√£o)

- [ ] **API FastAPI**: Endpoints `/search`, `/ingest`, `/validate`
- [ ] **Answer Generation**: Integrar retrieval + LLM (Qwen 3) para resposta
- [ ] **Prompts Jur√≠dicos**: Prompt especializado para resposta legal
- [ ] **Avalia√ß√£o RAGAS**: M√©tricas de qualidade (faithfulness, relevance)

### Longo Prazo (Produ√ß√£o)

- [ ] **UI Next.js**: Interface de busca com cita√ß√µes clic√°veis
- [ ] **PDF Viewer**: Clique na cita√ß√£o ‚Üí pula para p√°gina/coordenada
- [ ] **Multi-tenant**: Suporte a m√∫ltiplos √≥rg√£os/clientes
- [ ] **Observabilidade**: Logs, m√©tricas, tracing

---

## ‚úÖ Checklist de Produ√ß√£o

### Testes Obrigat√≥rios

- [x] Cobertura por tipo (PAR/INC) ‚â• 98% por artigo
- [x] Duplicatas = 0 por artigo e por chunk
- [x] Suffix‚Üîparent v√°lido (INC-005-II_2 ‚Üí parent=PAR-005-2)
- [ ] Round-trip: texto reconstru√≠do == concat dos spans
- [x] Retrieval contextual: pai sempre aparece no conjunto final
- [x] Answer-JSON: cita√ß√µes apontam para span_ids usados

### Governan√ßa

- [x] `schema_version`, `extractor_version` em cada chunk
- [x] `ingestion_timestamp`, `document_hash` para rastreabilidade
- [x] `page`, `bbox_*` para cita√ß√£o visual
- [x] `parent_chunk_id` para expans√£o de contexto

---

## üìÖ 23/12/2024 - Tarde/Noite: RAG Completo com Resposta LLM

### Resumo da Sess√£o

Nesta sess√£o completamos o ciclo RAG end-to-end:
1. **Celery Pipeline**: Enriquecimento paralelo de chunks
2. **Answer Generator**: Gera√ß√£o de respostas com cita√ß√µes
3. **Dashboard Streamlit**: Interface para perguntas
4. **Primeiro teste bem-sucedido**: Resposta 100% coerente com a lei

### 1. Pipeline Celery para Enriquecimento Paralelo

**Problema**: Lei 14.133 tem 1260 chunks. Enriquecer sequencialmente levaria ~5h.

**Solu√ß√£o**: Celery + Redis para processamento paralelo.

**Arquivos criados**:

```
src/enrichment/
‚îú‚îÄ‚îÄ __init__.py           # Exports do m√≥dulo
‚îú‚îÄ‚îÄ celery_app.py         # Configura√ß√£o Celery
‚îî‚îÄ‚îÄ tasks.py              # Tasks de enriquecimento

scripts/
‚îú‚îÄ‚îÄ run_enrichment_celery.py  # Dispara tasks
‚îî‚îÄ‚îÄ check_progress.py         # Monitora progresso
```

**celery_app.py** - Configura√ß√£o:
```python
from celery import Celery

app = Celery(
    "enrichment",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0",
    include=["src.enrichment.tasks"],
)

app.conf.update(
    task_time_limit=600,          # 10 min max por task
    task_default_rate_limit="10/m", # 10 tasks/min (respeita GPU)
    worker_prefetch_multiplier=1,   # 1 task por vez por worker
    task_acks_late=True,            # Retry se worker morrer
)
```

**tasks.py** - Task de enriquecimento:
```python
@app.task(bind=True, max_retries=3, default_retry_delay=30)
def enrich_chunk_task(self, chunk_id, text, device_type, ...):
    """Enriquece um chunk e atualiza no Milvus."""
    # 1. Inicializa LLM e enricher
    # 2. Gera context_header, thesis_text, synthetic_questions
    # 3. Gera novos embeddings com enriched_text
    # 4. Upsert no Milvus (delete + insert)
```

**Comandos para executar**:

```bash
# Terminal 1: Redis
docker run -d --name redis -p 6379:6379 redis:alpine

# Terminais 2-5: Workers Celery (4 workers)
cd extracao
celery -A src.enrichment.celery_app worker --loglevel=info --concurrency=1

# Terminal 6: Dispara tasks
python scripts/run_enrichment_celery.py

# Monitoramento
python scripts/check_progress.py --watch  # Atualiza a cada 30s
celery -A src.enrichment.celery_app flower  # Dashboard web :5555
```

**Resultado**:
- 4 workers processando em paralelo
- ~6-13 chunks/min (depende da complexidade)
- Taxa de sucesso: 100% (com retry autom√°tico)

### 2. Answer Generator - Resposta RAG com LLM

**M√≥dulo**: `src/rag/answer_generator.py`

**Fluxo completo**:
```
Query do usu√°rio
       ‚îÇ
       ‚ñº
[1. HyDE] LLM gera 3 documentos hipot√©ticos (opcional)
       ‚îÇ
       ‚ñº
[2. Embedding] BGE-M3 combina query + docs hipot√©ticos
       ‚îÇ
       ‚ñº
[3. Busca H√≠brida] Milvus (dense 50% + sparse 30% + thesis 20%)
       ‚îÇ
       ‚ñº
[4. Reranking] BGE-Reranker cross-encoder (optional)
       ‚îÇ
       ‚ñº
[5. Contexto] Monta chunks para prompt
       ‚îÇ
       ‚ñº
[6. Generation] Qwen 3 8B gera resposta com cita√ß√µes
       ‚îÇ
       ‚ñº
[7. Formata√ß√£o] Cita√ß√µes legais (Lei X, Art. Y, ¬ßZ)
```

**Uso**:
```python
from rag import AnswerGenerator, GenerationConfig

# Modo completo (HyDE + Reranker)
generator = AnswerGenerator()
response = generator.generate("Quais os crit√©rios de julgamento?")

print(response.answer)          # Resposta formatada
print(response.confidence)      # 0.999 (99.9%)
for citation in response.citations:
    print(citation.text)        # "Lei 14.133/2021, Art. 33, I"

# Modo r√°pido (sem HyDE, sem Reranker)
config = GenerationConfig.fast()
generator = AnswerGenerator(config=config)
```

**Estrutura da resposta (AnswerResponse)**:
```json
{
  "success": true,
  "query": "Quais os crit√©rios de julgamento?",
  "data": {
    "answer": "Os crit√©rios de julgamento previstos na Lei 14.133/2021 s√£o...",
    "confidence": 0.999,
    "citations": [
      {
        "text": "Lei 14.133/2021, Art. 33, I",
        "short": "Art. 33, I",
        "document_type": "Lei",
        "document_number": "14.133",
        "year": 2021,
        "article": "33",
        "device": "inciso",
        "device_number": "I"
      }
    ],
    "sources": [
      {"document_id": "LEI-14133-2021", "tipo_documento": "LEI", "ano": 2021}
    ]
  },
  "metadata": {
    "model": "Qwen/Qwen3-8B-AWQ",
    "latency_ms": 54650,
    "retrieval_ms": 25749,
    "generation_ms": 28900,
    "chunks_retrieved": 5,
    "chunks_used": 5
  }
}
```

### 3. Citation Formatter - Cita√ß√µes Legais

**M√≥dulo**: `src/rag/citation_formatter.py`

**Formata cita√ß√µes no padr√£o jur√≠dico brasileiro**:

| Tipo | Exemplo de Sa√≠da |
|------|------------------|
| Artigo | Lei 14.133/2021, Art. 33 |
| Par√°grafo | Lei 14.133/2021, Art. 14, Par. 5 |
| Inciso | Lei 14.133/2021, Art. 33, inciso I |
| Al√≠nea | Lei 14.133/2021, Art. 14, inciso II, al√≠nea 'a' |
| ¬ß √∫nico | IN 65/2021, Art. 3, Par√°grafo √∫nico |

**Uso**:
```python
from rag import CitationFormatter, format_citation

# Simples
citation = format_citation(
    tipo_documento="LEI",
    numero="14133",
    ano=2021,
    article_number="33",
    device_type="inciso",
    span_id="INC-033-I"
)
# -> "Lei 14.133/2021, Art. 33, inciso I"

# Com classe
formatter = CitationFormatter()
citation = formatter.format_from_chunk(chunk_data)
```

### 4. Dashboard Streamlit - P√°gina "Perguntar"

**Arquivo**: `src/dashboard/app.py`

**Nova p√°gina adicionada**: "Perguntar"

**Funcionalidades**:
- Campo de texto para perguntas
- Configura√ß√µes: HyDE, Reranker, Top-K
- Modo R√°pido vs Completo
- Resposta formatada do Qwen 3 8B
- Cita√ß√µes com artigo/par√°grafo/inciso
- M√©tricas de lat√™ncia (retrieval, generation, total)
- JSON completo para debug

**Como acessar**:
```bash
streamlit run src/dashboard/app.py --server.port 8501
# Acesse http://localhost:8501 ‚Üí p√°gina "Perguntar"
```

### 5. √çndices Milvus - Verifica√ß√£o de Uso

**Todos os √≠ndices vetoriais est√£o sendo utilizados** no modo HYBRID_3WAY:

| √çndice | Campo | Peso | Tipo | Status |
|--------|-------|------|------|--------|
| HNSW | `dense_vector` | 50% | COSINE | ‚úÖ Usado |
| HNSW | `thesis_vector` | 20% | COSINE | ‚úÖ Usado |
| SPARSE_INVERTED | `sparse_vector` | 30% | IP | ‚úÖ Usado |

**√çndices escalares** (usados em filtros):
- `tipo_documento` - INVERTED
- `ano` - INVERTED
- `article_number` - INVERTED
- `device_type` - INVERTED
- `parent_chunk_id` - INVERTED

### 6. Primeiro Teste Bem-Sucedido

**Query**: "Quais os crit√©rios de julgamento?"

**Resposta do sistema**:
```
Os crit√©rios de julgamento previstos na Lei 14.133/2021 s√£o os seguintes:

1. Menor pre√ßo ‚Äì [Art. 33, I].
2. Maior desconto ‚Äì [Art. 33, II].
3. Melhor t√©cnica ou conte√∫do art√≠stico ‚Äì [Art. 33, III].
4. T√©cnica e pre√ßo ‚Äì [Art. 33, IV].
5. Maior lance, no caso de leil√£o ‚Äì [Art. 33, V].
6. Maior retorno econ√¥mico ‚Äì [Art. 33, VI].

Detalhamento de alguns crit√©rios:
- Julgamento por t√©cnica e pre√ßo ([Art. 33, IV]):
  - Considera a pondera√ß√£o objetiva entre t√©cnica e pre√ßo, com at√© 70%
    da pontua√ß√£o atribu√≠da √† proposta t√©cnica ([Art. 36, ¬ß 2¬∫]).
...
```

**M√©tricas**:
- Confian√ßa: **99.9%**
- Retrieval: 25.7s
- Generation: 28.9s
- Total: **54.6s**

**Avalia√ß√£o**: Resposta **100% coerente** com a Lei 14.133/2021. Citou corretamente Art. 33 com todos os 6 crit√©rios e detalhou Art. 36 sobre t√©cnica e pre√ßo.

### 7. Progresso do Enriquecimento (em andamento)

| Documento | Chunks | Enriquecidos | Progresso |
|-----------|--------|--------------|-----------|
| IN 65/2021 | 47 | 47 | ‚úÖ 100% |
| Lei 14.133/2021 | 1260 | ~400 | ‚è≥ ~32% |
| **Total** | 1307 | ~447 | **~34%** |

Os 4 workers Celery continuam processando em background.

### 8. Arquivos Criados/Modificados (23/12/2024 tarde)

```
src/enrichment/
‚îú‚îÄ‚îÄ __init__.py              # NOVO: Exports
‚îú‚îÄ‚îÄ celery_app.py            # NOVO: Config Celery
‚îî‚îÄ‚îÄ tasks.py                 # NOVO: Tasks enriquecimento

src/rag/
‚îú‚îÄ‚îÄ __init__.py              # ATUALIZADO: Novos exports
‚îú‚îÄ‚îÄ answer_generator.py      # NOVO: Gera√ß√£o resposta RAG
‚îî‚îÄ‚îÄ citation_formatter.py    # NOVO: Formata√ß√£o cita√ß√µes

src/search/
‚îî‚îÄ‚îÄ models.py                # ATUALIZADO: Adicionado campo 'ano' e property 'year'

src/dashboard/
‚îî‚îÄ‚îÄ app.py                   # ATUALIZADO: Nova p√°gina "Perguntar"

scripts/
‚îú‚îÄ‚îÄ run_enrichment_celery.py # NOVO: Dispara tasks Celery
‚îú‚îÄ‚îÄ check_progress.py        # NOVO: Monitora progresso
‚îî‚îÄ‚îÄ test_answer_generator.py # NOVO: Teste do generator
```

### 9. Li√ß√µes Aprendidas

| Li√ß√£o | Contexto |
|-------|----------|
| **Celery precisa de imports corretos** | Usar `src.llm.vllm_client` ao inv√©s de `llm.vllm_client` |
| **Milvus insert row-oriented** | Usar `[{campo: valor}]` ao inv√©s de `{campo: [valor]}` |
| **Streamlit cache √© agressivo** | Reiniciar processo para pegar mudan√ßas em m√≥dulos |
| **HyDE adiciona ~15-20s** | Desativar para queries simples |
| **Reranker adiciona ~10s** | Mas melhora precis√£o significativamente |
| **Qualidade > Velocidade** | Primeiro garantir respostas corretas, depois otimizar |

### 10. M√©tricas de Lat√™ncia

| Modo | HyDE | Reranker | Retrieval | Generation | Total |
|------|------|----------|-----------|------------|-------|
| R√°pido | ‚ùå | ‚ùå | ~11s | ~19s | **~30s** |
| Completo | ‚úÖ | ‚úÖ | ~26s | ~29s | **~55s** |

**Causas da lat√™ncia**:
- HyDE: LLM gera 3 documentos hipot√©ticos (~15s)
- Reranker: Cross-encoder processa top-20 (~10s)
- Generation: Resposta longa com cita√ß√µes (~20-30s)

---

## üéØ Status Atual do Projeto (23/12/2024 21:30)

### Fase Atual: **5 - RAG Completo com Resposta LLM** ‚úÖ

| Componente | Status | Descri√ß√£o |
|------------|--------|-----------|
| Extra√ß√£o PDF | ‚úÖ Completo | Docling + SpanParser + ArticleOrchestrator |
| Chunking | ‚úÖ Completo | ChunkMaterializer com parent-child |
| Embeddings | ‚úÖ Completo | BGE-M3 (dense + sparse) |
| Enriquecimento | ‚è≥ Em andamento | ChunkEnricher (32% Lei 14.133) |
| Indexa√ß√£o | ‚úÖ Completo | Milvus leis_v3 (1307 chunks) |
| Busca H√≠brida | ‚úÖ Completo | Weighted 3-way + HyDE |
| Reranking | ‚úÖ Completo | BGE-Reranker cross-encoder |
| Resposta LLM | ‚úÖ Completo | AnswerGenerator + Qwen 8B |
| Cita√ß√µes | ‚úÖ Completo | CitationFormatter |
| Dashboard | ‚úÖ Completo | Streamlit com p√°gina "Perguntar" |

### O que funciona end-to-end

```
Usu√°rio faz pergunta
        ‚îÇ
        ‚ñº
[Dashboard Streamlit] ‚Üí [AnswerGenerator]
        ‚îÇ
        ‚ñº
[HyDE opcional] ‚Üí [Busca H√≠brida Milvus] ‚Üí [Reranker]
        ‚îÇ
        ‚ñº
[Monta contexto com chunks] ‚Üí [Qwen 3 8B gera resposta]
        ‚îÇ
        ‚ñº
[Formata cita√ß√µes] ‚Üí [Exibe resposta + m√©tricas]
```

---

## üöÄ Pr√≥ximos Passos (Atualizado)

### Conclu√≠do (23/12/2024)

- [x] **Pipeline Celery**: Enriquecimento paralelo com 4 workers
- [x] **Answer Generator**: Gera√ß√£o de respostas RAG com cita√ß√µes
- [x] **Citation Formatter**: Formata√ß√£o de cita√ß√µes legais
- [x] **Dashboard "Perguntar"**: Interface para perguntas ao sistema
- [x] **Primeiro teste bem-sucedido**: Resposta 100% coerente

### Curto Prazo (pr√≥xima sess√£o)

- [ ] **Completar enriquecimento Lei 14.133**: Aguardar Celery (~4h restantes)
- [ ] **Otimizar lat√™ncia**: Cache de embeddings, streaming response
- [ ] **API FastAPI**: Endpoints `/ask`, `/search`, `/ingest`
- [ ] **Streaming response**: Mostrar resposta enquanto gera

### M√©dio Prazo

- [ ] **Cache de queries**: Perguntas frequentes pr√©-computadas
- [ ] **Avalia√ß√£o RAGAS**: M√©tricas de qualidade (faithfulness, relevance)
- [ ] **Mais documentos**: Indexar Decretos, outras Leis
- [ ] **Fine-tuning prompts**: Melhorar precis√£o das respostas

### Longo Prazo (Produ√ß√£o)

- [ ] **UI React/Next.js**: Interface profissional
- [ ] **PDF Viewer**: Clique na cita√ß√£o ‚Üí pula para p√°gina
- [ ] **Multi-tenant**: Suporte a m√∫ltiplos √≥rg√£os
- [ ] **GPU maior**: RTX 4090 para lat√™ncia 2x menor
- [ ] **Kubernetes**: Deploy escal√°vel

---

## üìä Comandos √öteis

### Iniciar Sistema Completo

```bash
# 1. Docker (Milvus + vLLM)
docker start milvus-standalone vllm

# 2. Redis (para Celery)
docker run -d --name redis -p 6379:6379 redis:alpine

# 3. Workers Celery (abrir 4 terminais)
cd extracao
celery -A src.enrichment.celery_app worker --loglevel=info --concurrency=1

# 4. Dashboard Streamlit
streamlit run src/dashboard/app.py --server.port 8501
```

### Monitoramento

```bash
# Progresso do enriquecimento
python scripts/check_progress.py --watch

# Dashboard Celery (Flower)
celery -A src.enrichment.celery_app flower
# Acesse http://localhost:5555

# Logs do vLLM
docker logs -f vllm
```

### Testar Resposta RAG

```bash
# Via linha de comando
python scripts/test_answer_generator.py --query "Quando o ETP pode ser dispensado?"

# Modo r√°pido (sem HyDE)
python scripts/test_answer_generator.py --fast --query "Quais os crit√©rios de julgamento?"
```

---

## üîó Refer√™ncias

- [Docling Documentation](https://ds4sd.github.io/docling/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [BGE-M3 Model](https://huggingface.co/BAAI/bge-m3)
- [Milvus 2.6 Documentation](https://milvus.io/docs)
- [Qwen 3 Models](https://huggingface.co/Qwen)
- [Ollama](https://ollama.com/) - Runtime local para LLMs
- [LlamaExtract](https://developers.llamaindex.ai/python/cloud/llamaextract/) - Inspira√ß√£o para API
- [vLLM](https://docs.vllm.ai/) - Runtime de produ√ß√£o
- [Celery Documentation](https://docs.celeryq.dev/) - Task queue
- [Streamlit Documentation](https://docs.streamlit.io/) - Dashboard
